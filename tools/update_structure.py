#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›®å½•ç»“æ„æ›´æ–°å·¥å…·

åŠŸèƒ½:
- æ‰«æé¡¹ç›®ç›®å½•ç»“æ„
- ç”Ÿæˆæ ‡å‡†åŒ–çš„ç›®å½•ç»“æ„æ¸…å•
- æ”¯æŒæ’é™¤ç‰¹å®šç›®å½•å’Œæ–‡ä»¶
- ç”ŸæˆMarkdownæ ¼å¼çš„ç»“æ„æ–‡æ¡£

ä½œè€…: é›¨ä¿Š
åˆ›å»ºæ—¶é—´: 2025-07-08
æœ€åæ›´æ–°: 2025-07-08
"""

import sys
from pathlib import Path
from typing import List, Dict
from datetime import datetime
import argparse
import yaml
import asyncio

# import aiofiles  # æš‚æ—¶ä¸ä½¿ç”¨å¼‚æ­¥æ–‡ä»¶æ“ä½œ
from concurrent.futures import ThreadPoolExecutor
import time
import json
import xml.etree.ElementTree as ET
from xml.dom import minidom
import hashlib


# å¯¼å…¥å·¥å…·æ¨¡å—
from utils import get_project_root

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


class DirectoryStructureGenerator:
    """ç›®å½•ç»“æ„ç”Ÿæˆå™¨"""

    def __init__(self):
        # è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
        self.project_root = get_project_root()

        # åŠ è½½é…ç½®æ–‡ä»¶
        self.config = self._load_config()

        # ä»é…ç½®æ–‡ä»¶ä¸­è·å–ç”Ÿæˆå™¨é…ç½®
        structure_config = self.config.get("structure_check", {})
        generator_config = structure_config.get("generator", {})

        # æ’é™¤è§„åˆ™é…ç½®
        self.excluded_dirs = set(
            generator_config.get(
                "excluded_dirs",
                [
                    "__pycache__",
                    ".git",
                    ".vscode",
                    ".idea",
                    "node_modules",
                    ".pytest_cache",
                    ".coverage",
                    "htmlcov",
                    "dist",
                    "build",
                    "*.egg-info",
                    ".tox",
                    ".mypy_cache",
                    ".DS_Store",
                    "Thumbs.db",
                    ".venv",
                    "venv",
                    "env",
                ],
            )
        )

        self.excluded_files = set(
            generator_config.get(
                "excluded_files",
                [
                    ".gitkeep",
                    ".DS_Store",
                    "Thumbs.db",
                    "*.pyc",
                    "*.pyo",
                    "*.pyd",
                    "__pycache__",
                    "*.so",
                    "*.dylib",
                    "*.dll",
                ],
            )
        )

        # å…è®¸çš„éšè—æ–‡ä»¶/ç›®å½•
        self.allowed_hidden_items = set(
            generator_config.get(
                "allowed_hidden_items",
                [
                    ".env",
                    ".env.example",
                    ".gitignore",
                    ".dockerignore",
                    ".eslintrc.js",
                    ".prettierrc",
                    ".pre-commit-config.yaml",
                    ".devcontainer",
                    ".github",
                    ".venv",
                ],
            )
        )

        # ç‰¹æ®Šç›®å½•é…ç½®
        self.special_dirs = generator_config.get(
            "special_dirs",
            {
                "bak": [
                    "github_repo",
                    "è¿ç§»å¤‡ä»½",
                    "ä¸“é¡¹å¤‡ä»½",
                    "å¾…æ¸…ç†èµ„æ–™",
                    "å¸¸è§„å¤‡ä»½",
                ],
                "logs": ["å·¥ä½œè®°å½•", "æ£€æŸ¥æŠ¥å‘Š", "å…¶ä»–æ—¥å¿—", "archive"],
            },
        )

        # è¾“å‡ºæ ¼å¼é…ç½®
        self.output_formats = generator_config.get("output_formats", ["markdown"])

        # æ€§èƒ½é…ç½®
        self.performance = generator_config.get(
            "performance", {"max_workers": 4, "batch_size": 100, "enable_async": True}
        )

        # ç¼“å­˜é…ç½®
        self.cache = generator_config.get(
            "cache",
            {
                "enabled": False,
                "cache_file": "structure_cache.json",
                "ttl_hours": 24,
                "cache_dir": ".cache/structure",
                "check_mtime": True,
            },
        )

        # åˆå§‹åŒ–ç¼“å­˜ç›®å½•
        if self.cache.get("enabled", False):
            self._init_cache_dir()

        # æ€§èƒ½ç»Ÿè®¡
        self.perf_stats = {
            "scan_start_time": None,
            "scan_end_time": None,
            "total_scan_time": 0,
            "async_enabled": self.performance.get("enable_async", True),
        }

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {"total_dirs": 0, "total_files": 0}

    def _init_cache_dir(self) -> None:
        """åˆå§‹åŒ–ç¼“å­˜ç›®å½•"""
        cache_dir_str = self.cache.get("cache_dir", ".cache/structure")
        cache_dir = self.project_root / Path(cache_dir_str)
        cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_cache_file_path(self) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_dir_str = self.cache.get("cache_dir", ".cache/structure")
        cache_dir = self.project_root / Path(cache_dir_str)
        cache_file = self.cache.get("cache_file", "structure_cache.json")
        return cache_dir / cache_file

    def _get_directory_hash(self, dir_path: Path) -> str:
        """è®¡ç®—ç›®å½•çš„å“ˆå¸Œå€¼ï¼ˆåŸºäºè·¯å¾„å’Œä¿®æ”¹æ—¶é—´ï¼‰

        Args:
            dir_path: ç›®å½•è·¯å¾„

        Returns:
            ç›®å½•çš„å“ˆå¸Œå€¼
        """
        try:
            # è·å–ç›®å½•çš„ä¿®æ”¹æ—¶é—´
            mtime = dir_path.stat().st_mtime if dir_path.exists() else 0

            # è®¡ç®—å“ˆå¸Œå€¼
            hash_data = f"{dir_path}:{mtime}"
            return hashlib.md5(hash_data.encode()).hexdigest()
        except Exception:
            return hashlib.md5(str(dir_path).encode()).hexdigest()

    def _load_cache(self) -> Dict:
        """åŠ è½½ç¼“å­˜æ•°æ®

        Returns:
            ç¼“å­˜æ•°æ®å­—å…¸
        """
        if not self.cache.get("enabled", False):
            return {}

        cache_file = self._get_cache_file_path()
        if not cache_file.exists():
            return {}

        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                cache_data = json.load(f)

            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            cache_time = cache_data.get("timestamp", 0)
            ttl_seconds = self.cache.get("ttl_hours", 24) * 3600
            current_time = time.time()

            if current_time - cache_time > ttl_seconds:
                print("ğŸ—‘ï¸  ç¼“å­˜å·²è¿‡æœŸï¼Œå°†é‡æ–°æ‰«æ")
                return {}

            return cache_data
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return {}

    def _save_cache(
        self, structure: List[Dict], directory_hashes: Dict[str, str]
    ) -> None:
        """ä¿å­˜ç¼“å­˜æ•°æ®

        Args:
            structure: ç›®å½•ç»“æ„æ•°æ®
            directory_hashes: ç›®å½•å“ˆå¸Œå€¼æ˜ å°„
        """
        if not self.cache.get("enabled", False):
            return

        cache_file = self._get_cache_file_path()

        try:
            cache_data = {
                "timestamp": time.time(),
                "structure": structure,
                "directory_hashes": directory_hashes,
                "stats": self.stats.copy(),
            }

            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {cache_file}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _is_directory_changed(self, dir_path: Path, cached_hash: str) -> bool:
        """æ£€æŸ¥ç›®å½•æ˜¯å¦å‘ç”Ÿå˜åŒ–

        Args:
            dir_path: ç›®å½•è·¯å¾„
            cached_hash: ç¼“å­˜çš„å“ˆå¸Œå€¼

        Returns:
            ç›®å½•æ˜¯å¦å‘ç”Ÿå˜åŒ–
        """
        if not self.cache.get("check_mtime", True):
            return False

        current_hash = self._get_directory_hash(dir_path)
        return current_hash != cached_hash

    def _merge_cached_structure(
        self, cached_structure: List[Dict], new_structure: List[Dict]
    ) -> List[Dict]:
        """åˆå¹¶ç¼“å­˜çš„ç»“æ„å’Œæ–°æ‰«æçš„ç»“æ„

        Args:
            cached_structure: ç¼“å­˜çš„ç»“æ„æ•°æ®
            new_structure: æ–°æ‰«æçš„ç»“æ„æ•°æ®

        Returns:
            åˆå¹¶åçš„ç»“æ„æ•°æ®
        """
        # åˆ›å»ºè·¯å¾„åˆ°ç»“æ„é¡¹çš„æ˜ å°„
        cached_map = {item["path"]: item for item in cached_structure}
        new_map = {item["path"]: item for item in new_structure}

        # åˆå¹¶ç»“æ„
        merged = []
        all_paths = set(cached_map.keys()) | set(new_map.keys())

        for path in sorted(all_paths):
            if path in new_map:
                # ä½¿ç”¨æ–°æ‰«æçš„æ•°æ®
                merged.append(new_map[path])
            elif path in cached_map:
                # ä½¿ç”¨ç¼“å­˜çš„æ•°æ®
                merged.append(cached_map[path])

        return merged

    def _load_config(self) -> Dict:
        """åŠ è½½é¡¹ç›®é…ç½®æ–‡ä»¶"""
        try:
            project_root = get_project_root()
            config_file = (
                Path(project_root) / "docs" / "03-ç®¡ç†" / "project_config.yaml"
            )

            if config_file.exists():
                with open(config_file, "r", encoding="utf-8") as f:
                    return yaml.safe_load(f) or {}
            else:
                print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
                return {}
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}

    def should_exclude(self, path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ’é™¤æŸä¸ªè·¯å¾„"""

        # æ’é™¤ç‰¹å®šç›®å½•
        if path.name in self.excluded_dirs:
            return True

        # æ’é™¤ç‰¹å®šæ–‡ä»¶
        if path.is_file() and path.name in self.excluded_files:
            return True

        return False

    def should_filter_special_directory(self, relative_path: str, entry: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿‡æ»¤ç‰¹æ®Šç›®å½•ä¸­çš„é¡¹ç›®"""

        # ä»é…ç½®ä¸­è·å–å…è®¸çš„å­ç›®å½•
        allowed_bak_dirs = set(self.special_dirs.get("bak", []))
        allowed_logs_dirs = set(self.special_dirs.get("logs", []))

        # æ£€æŸ¥æ˜¯å¦åœ¨bak/ç›®å½•ä¸‹
        if relative_path.startswith("bak/"):
            # å¦‚æœæ˜¯bak/ä¸‹çš„ç›´æ¥å­é¡¹ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­
            if relative_path.count("/") == 1:  # bak/xxx æ ¼å¼
                dir_name = relative_path.split("/")[-1]
                if entry.is_dir() and dir_name not in allowed_bak_dirs:
                    return True  # è¿‡æ»¤æ‰ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­çš„ç›®å½•
                elif entry.is_file():
                    return True  # è¿‡æ»¤æ‰bak/ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
            elif relative_path.count("/") > 1:  # bak/xxx/yyy æ ¼å¼
                return True  # è¿‡æ»¤æ‰bak/å­ç›®å½•ä¸‹çš„æ‰€æœ‰å†…å®¹

        # æ£€æŸ¥æ˜¯å¦åœ¨logs/ç›®å½•ä¸‹
        elif relative_path.startswith("logs/"):
            # å¦‚æœæ˜¯logs/ä¸‹çš„ç›´æ¥å­é¡¹ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­
            if relative_path.count("/") == 1:  # logs/xxx æ ¼å¼
                dir_name = relative_path.split("/")[-1]
                if entry.is_dir() and dir_name not in allowed_logs_dirs:
                    return True  # è¿‡æ»¤æ‰ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­çš„ç›®å½•
                elif entry.is_file():
                    return True  # è¿‡æ»¤æ‰logs/ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
            elif relative_path.count("/") > 1:  # logs/xxx/yyy æ ¼å¼
                return True  # è¿‡æ»¤æ‰logs/å­ç›®å½•ä¸‹çš„æ‰€æœ‰å†…å®¹

        return False

    def scan_filtered_directory(self, dir_path: Path, relative_path: str) -> List[Dict]:
        """æ‰«æç»è¿‡ç‰¹æ®Šè¿‡æ»¤çš„ç›®å½•ï¼ˆbak/å’Œlogs/ï¼‰"""
        items = []

        # ä»é…ç½®ä¸­è·å–å…è®¸çš„å­ç›®å½•
        if relative_path == "bak":
            allowed_dirs = set(self.special_dirs.get("bak", []))
        elif relative_path == "logs":
            allowed_dirs = set(self.special_dirs.get("logs", []))
        else:
            return items

        try:
            # åªæ‰«æå…è®¸çš„å­ç›®å½•
            entries = sorted(dir_path.iterdir(), key=lambda x: x.name.lower())
            for entry in entries:
                if entry.is_dir() and entry.name in allowed_dirs:
                    # æ·»åŠ å…è®¸çš„å­ç›®å½•ï¼Œä½†ä¸é€’å½’æ‰«æå…¶å†…å®¹
                    items.append(
                        {
                            "type": "directory",
                            "name": entry.name,
                            "path": f"{relative_path}/{entry.name}/",
                            "children": [],  # ä¸æ‰«æå­ç›®å½•å†…å®¹
                        }
                    )
                    self.stats["total_dirs"] += 1
        except Exception as e:
            print(f"âŒ æ‰«æè¿‡æ»¤ç›®å½•æ—¶å‡ºé”™: {dir_path} - {type(e).__name__}: {e}")

        return items

    async def scan_directory_async(
        self,
        dir_path: Path,
        relative_path: str = "",
        semaphore: asyncio.Semaphore = None,
        executor: ThreadPoolExecutor = None,
    ) -> List[Dict]:
        """å¼‚æ­¥æ‰«æç›®å½•ç»“æ„

        Args:
            dir_path: è¦æ‰«æçš„ç›®å½•è·¯å¾„
            relative_path: ç›¸å¯¹è·¯å¾„å‰ç¼€
            semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡

        Returns:
            ç›®å½•ç»“æ„åˆ—è¡¨
        """
        if semaphore is None:
            semaphore = asyncio.Semaphore(self.performance.get("max_workers", 4))

        if executor is None:
            executor = ThreadPoolExecutor(
                max_workers=self.performance.get("max_workers", 4)
            )
            should_close_executor = True
        else:
            should_close_executor = False

        items = []

        try:
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥çš„ç›®å½•è¯»å–æ“ä½œ
            loop = asyncio.get_event_loop()
            entries = await loop.run_in_executor(executor, list, dir_path.iterdir())

            # æŒ‰åç§°æ’åºï¼Œç›®å½•åœ¨å‰
            entries.sort(key=lambda x: (x.is_file(), x.name.lower()))

            # åˆ†æ‰¹å¤„ç†æ¡ç›®ä»¥é¿å…è¿‡å¤šå¹¶å‘
            batch_size = self.performance.get("batch_size", 100)
            for i in range(0, len(entries), batch_size):
                batch = entries[i : i + batch_size]
                batch_tasks = []

                for entry in batch:
                    if self.should_exclude(entry):
                        continue

                    # æ„å»ºç›¸å¯¹è·¯å¾„
                    if relative_path:
                        item_relative_path = f"{relative_path}/{entry.name}"
                    else:
                        item_relative_path = entry.name

                    # ç‰¹æ®Šå¤„ç†bak/å’Œlogs/ç›®å½•
                    if self.should_filter_special_directory(item_relative_path, entry):
                        continue

                    # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
                    task = self._process_entry_async(
                        entry, item_relative_path, semaphore, executor
                    )
                    batch_tasks.append(task)

                # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
                if batch_tasks:
                    batch_results = await asyncio.gather(
                        *batch_tasks, return_exceptions=True
                    )
                    for result in batch_results:
                        if isinstance(result, Exception):
                            print(f"âš ï¸  å¤„ç†æ¡ç›®æ—¶å‡ºé”™: {result}")
                        elif result is not None:
                            items.append(result)

        except Exception as e:
            print(f"âŒ å¼‚æ­¥æ‰«æç›®å½•æ—¶å‡ºé”™: {dir_path} - {type(e).__name__}: {e}")
        finally:
            # å¦‚æœæ˜¯æˆ‘ä»¬åˆ›å»ºçš„executorï¼Œéœ€è¦å…³é—­å®ƒ
            if should_close_executor:
                executor.shutdown(wait=True)

        return items

    async def _process_entry_async(
        self,
        entry: Path,
        item_relative_path: str,
        semaphore: asyncio.Semaphore,
        executor: ThreadPoolExecutor,
    ) -> Dict:
        """å¼‚æ­¥å¤„ç†å•ä¸ªæ¡ç›®

        Args:
            entry: æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
            item_relative_path: ç›¸å¯¹è·¯å¾„
            semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
            executor: å…±äº«çš„çº¿ç¨‹æ± æ‰§è¡Œå™¨

        Returns:
            æ¡ç›®ä¿¡æ¯å­—å…¸
        """
        async with semaphore:
            try:
                if entry.is_dir():
                    # ç›®å½•å¤„ç†
                    self.stats["total_dirs"] += 1

                    # å¯¹äºbak/å’Œlogs/ç›®å½•ï¼Œä½¿ç”¨åŒæ­¥æ–¹æ³•
                    if item_relative_path == "bak" or item_relative_path == "logs":
                        children = self.scan_filtered_directory(
                            entry, item_relative_path
                        )
                    else:
                        children = await self.scan_directory_async(
                            entry, item_relative_path, semaphore, executor
                        )

                    return {
                        "type": "directory",
                        "name": entry.name,
                        "path": item_relative_path,
                        "children": children,
                    }
                else:
                    # æ–‡ä»¶å¤„ç†
                    self.stats["total_files"] += 1

                    # å¼‚æ­¥è·å–æ–‡ä»¶å¤§å°
                    file_size = await self._get_file_size_async(entry, executor)

                    return {
                        "type": "file",
                        "name": entry.name,
                        "path": item_relative_path,
                        "size": file_size,
                    }

            except Exception as e:
                print(f"âš ï¸  å¤„ç†æ¡ç›®æ—¶å‡ºé”™: {entry} - {type(e).__name__}: {e}")
                return None

    async def _get_file_size_async(
        self, file_path: Path, executor: ThreadPoolExecutor
    ) -> int:
        """å¼‚æ­¥è·å–æ–‡ä»¶å¤§å°

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            executor: å…±äº«çš„çº¿ç¨‹æ± æ‰§è¡Œå™¨

        Returns:
            æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œå¤±è´¥æ—¶è¿”å›-1
        """
        try:
            loop = asyncio.get_event_loop()
            stat_result = await loop.run_in_executor(
                executor, lambda: file_path.stat() if file_path.exists() else None
            )
            return stat_result.st_size if stat_result else -1
        except Exception as e:
            print(f"âš ï¸  å¼‚æ­¥è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {file_path} - {e}")
            return -1

    def scan_directory(self, dir_path: Path, relative_path: str = "") -> List[Dict]:
        """æ‰«æç›®å½•ç»“æ„

        Args:
            dir_path: è¦æ‰«æçš„ç›®å½•è·¯å¾„
            relative_path: ç›¸å¯¹è·¯å¾„å‰ç¼€

        Returns:
            ç›®å½•ç»“æ„åˆ—è¡¨
        """
        items = []

        try:
            # è·å–ç›®å½•ä¸‹æ‰€æœ‰é¡¹ç›®
            entries = list(dir_path.iterdir())
            # æŒ‰åç§°æ’åºï¼Œç›®å½•åœ¨å‰
            entries.sort(key=lambda x: (x.is_file(), x.name.lower()))

            for entry in entries:
                if self.should_exclude(entry):
                    continue

                # æ„å»ºç›¸å¯¹è·¯å¾„
                if relative_path:
                    item_relative_path = f"{relative_path}/{entry.name}"
                else:
                    item_relative_path = entry.name

                # ç‰¹æ®Šå¤„ç†bak/å’Œlogs/ç›®å½•ï¼Œåªæ˜¾ç¤ºæŒ‡å®šçš„å­ç›®å½•å
                if self.should_filter_special_directory(item_relative_path, entry):
                    continue

                if entry.is_dir():
                    # ç›®å½•
                    self.stats["total_dirs"] += 1

                    # å¯¹äºbak/å’Œlogs/ç›®å½•ï¼Œåªæ‰«æå…è®¸çš„å­ç›®å½•
                    children = []
                    if item_relative_path == "bak" or item_relative_path == "logs":
                        children = self.scan_filtered_directory(
                            entry, item_relative_path
                        )
                    else:
                        children = self.scan_directory(entry, item_relative_path)

                    item = {
                        "type": "directory",
                        "name": entry.name,
                        "path": item_relative_path,
                        "children": children,
                    }
                    items.append(item)

                else:
                    # æ–‡ä»¶
                    self.stats["total_files"] += 1

                    # å®‰å…¨è·å–æ–‡ä»¶å¤§å°
                    file_size = 0
                    try:
                        if entry.exists():
                            file_size = entry.stat().st_size
                    except (PermissionError, OSError, FileNotFoundError) as e:
                        print(f"âš ï¸  æ— æ³•è·å–æ–‡ä»¶å¤§å°: {entry} - {e}")
                        file_size = -1  # æ ‡è®°ä¸ºæ— æ³•è·å–
                    except Exception as e:
                        print(
                            f"âš ï¸  è·å–æ–‡ä»¶ä¿¡æ¯æ—¶å‡ºé”™: {entry} - "
                            f"{type(e).__name__}: {e}"
                        )
                        file_size = -1

                    item = {
                        "type": "file",
                        "name": entry.name,
                        "path": item_relative_path,
                        "size": file_size,
                    }
                    items.append(item)

        except PermissionError as e:
            print(f"âš ï¸  æƒé™ä¸è¶³ï¼Œè·³è¿‡ç›®å½•: {dir_path} - {e}")
        except FileNotFoundError as e:
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {dir_path} - {e}")
        except OSError as e:
            if e.errno == 36:  # æ–‡ä»¶åè¿‡é•¿
                print(f"âš ï¸  æ–‡ä»¶åè¿‡é•¿ï¼Œè·³è¿‡ç›®å½•: {dir_path}")
            elif e.errno == 2:  # æ–‡ä»¶ä¸å­˜åœ¨
                print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {dir_path}")
            elif e.errno == 13:  # æƒé™æ‹’ç»
                print(f"âš ï¸  è®¿é—®è¢«æ‹’ç»ï¼Œè·³è¿‡ç›®å½•: {dir_path}")
            else:
                print(f"âŒ ç³»ç»Ÿé”™è¯¯ï¼Œè·³è¿‡ç›®å½•: {dir_path} - {e}")
        except UnicodeDecodeError as e:
            print(f"âš ï¸  ç¼–ç é”™è¯¯ï¼Œè·³è¿‡ç›®å½•: {dir_path} - {e}")
        except RecursionError as e:
            print(f"âŒ é€’å½’æ·±åº¦è¶…é™ï¼Œè·³è¿‡ç›®å½•: {dir_path} - {e}")
        except MemoryError as e:
            print(f"âŒ å†…å­˜ä¸è¶³ï¼Œè·³è¿‡ç›®å½•: {dir_path} - {e}")
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯ï¼Œè·³è¿‡ç›®å½•: {dir_path} - {type(e).__name__}: {e}")

        return items

    async def scan_directory_with_performance(self, dir_path: Path) -> List[Dict]:
        """å¸¦æ€§èƒ½ä¼˜åŒ–çš„ç›®å½•æ‰«æå…¥å£æ–¹æ³•

        Args:
            dir_path: è¦æ‰«æçš„ç›®å½•è·¯å¾„

        Returns:
            ç›®å½•ç»“æ„åˆ—è¡¨
        """
        # è®°å½•å¼€å§‹æ—¶é—´
        self.perf_stats["scan_start_time"] = time.time()

        # å°è¯•åŠ è½½ç¼“å­˜
        cache_data = self._load_cache()
        cached_structure = cache_data.get("structure", [])
        cached_hashes = cache_data.get("directory_hashes", {})

        if cache_data and cached_structure:
            print("ğŸ“¦ å‘ç°ç¼“å­˜æ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°...")

            # æ£€æŸ¥æ ¹ç›®å½•æ˜¯å¦å‘ç”Ÿå˜åŒ–
            root_hash = cached_hashes.get(str(dir_path), "")
            if not self._is_directory_changed(dir_path, root_hash):
                print("âœ… ç¼“å­˜æœ‰æ•ˆï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜æ•°æ®")
                # æ¢å¤ç»Ÿè®¡ä¿¡æ¯
                if "stats" in cache_data:
                    self.stats.update(cache_data["stats"])

                end_time = time.time()
                scan_start = self.perf_stats["scan_start_time"]
                print(f"â±ï¸  ç¼“å­˜åŠ è½½è€—æ—¶: {end_time - scan_start:.2f}ç§’")
                print(
                    f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: {self.stats['total_dirs']}ä¸ªç›®å½•, "
                    f"{self.stats['total_files']}ä¸ªæ–‡ä»¶"
                )
                return cached_structure
            else:
                print("ğŸ”„ æ£€æµ‹åˆ°ç›®å½•å˜åŒ–ï¼Œæ‰§è¡Œå¢é‡æ‰«æ...")

        print(
            f"ğŸš€ å¯åŠ¨{'å¼‚æ­¥' if self.perf_stats['async_enabled'] else 'åŒæ­¥'}æ‰«ææ¨¡å¼"
        )
        print(
            f"âš™ï¸  é…ç½®: æœ€å¤§å·¥ä½œçº¿ç¨‹={self.performance.get('max_workers', 4)}, "
            f"æ‰¹å¤„ç†å¤§å°={self.performance.get('batch_size', 100)}"
        )

        try:
            if self.perf_stats["async_enabled"]:
                # ä½¿ç”¨å¼‚æ­¥æ‰«æ
                structure = await self.scan_directory_async(dir_path)
            else:
                # ä½¿ç”¨åŒæ­¥æ‰«æ
                structure = self.scan_directory(dir_path)

            # è®°å½•ç»“æŸæ—¶é—´
            self.perf_stats["scan_end_time"] = time.time()
            self.perf_stats["total_scan_time"] = (
                self.perf_stats["scan_end_time"] - self.perf_stats["scan_start_time"]
            )

            print(f"â±ï¸  æ‰«æè€—æ—¶: {self.perf_stats['total_scan_time']:.2f}ç§’")
            print(
                f"ğŸ“Š æ‰«æç»Ÿè®¡: {self.stats['total_dirs']}ä¸ªç›®å½•, "
                f"{self.stats['total_files']}ä¸ªæ–‡ä»¶"
            )

            # ä¿å­˜ç¼“å­˜
            if self.cache.get("enabled", False):
                directory_hashes = {str(dir_path): self._get_directory_hash(dir_path)}
                self._save_cache(structure, directory_hashes)

            return structure

        except Exception as e:
            print(f"âŒ æ‰«æå¤±è´¥: {type(e).__name__}: {e}")
            raise

    def generate_markdown(
        self, structure: List[Dict], title: str = "é¡¹ç›®ç›®å½•ç»“æ„"
    ) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„ç›®å½•ç»“æ„

        Args:
            structure: ç›®å½•ç»“æ„æ•°æ®
            title: æ–‡æ¡£æ ‡é¢˜

        Returns:
            Markdownæ ¼å¼çš„å­—ç¬¦ä¸²
        """
        lines = []
        lines.append(f"# {title}")
        lines.append("")
        lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        lines.append("## ç›®å½•ç»“æ„")
        lines.append("")
        lines.append("```")

        def generate_tree(
            items: List[Dict], prefix: str = "", is_last_list: List[bool] = None
        ) -> None:
            """ç”Ÿæˆç›®å½•æ ‘ç»“æ„"""
            if is_last_list is None:
                is_last_list = []

            for i, item in enumerate(items):
                is_last = i == len(items) - 1

                # æ„å»ºå½“å‰è¡Œçš„å‰ç¼€
                current_prefix = ""
                for j, is_last_parent in enumerate(is_last_list):
                    if j == len(is_last_list) - 1:
                        continue
                    current_prefix += "â”‚   " if not is_last_parent else "    "

                # æ·»åŠ å½“å‰é¡¹çš„è¿æ¥ç¬¦
                if is_last_list:
                    current_prefix += "â””â”€â”€ " if is_last else "â”œâ”€â”€ "

                # è¾“å‡ºå½“å‰é¡¹
                if item["type"] == "directory":
                    lines.append(f"{current_prefix}{item['name']}/")
                    # é€’å½’å¤„ç†å­ç›®å½•
                    children = item.get("children", [])
                    if children:
                        new_is_last_list = is_last_list + [is_last]
                        generate_tree(children, current_prefix, new_is_last_list)
                else:
                    lines.append(f"{current_prefix}{item['name']}")

        # ç”Ÿæˆç›®å½•æ ‘
        generate_tree(structure)

        lines.append("```")
        lines.append("")

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        lines.append("## ç»Ÿè®¡ä¿¡æ¯")
        lines.append("")
        lines.append(f"- **ç›®å½•æ•°é‡**: {self.stats['total_dirs']}")
        lines.append(f"- **æ–‡ä»¶æ•°é‡**: {self.stats['total_files']}")
        lines.append("")

        # æ·»åŠ è¯´æ˜
        lines.append("## è¯´æ˜")
        lines.append("")
        lines.append("- æ­¤æ–‡æ¡£ç”±ç›®å½•ç»“æ„æ›´æ–°å·¥å…·è‡ªåŠ¨ç”Ÿæˆ")
        lines.append("- å·²æ’é™¤å¸¸è§çš„ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜ç›®å½•")
        lines.append("- ç›®å½•è·¯å¾„ä»¥ / ç»“å°¾ï¼Œæ–‡ä»¶è·¯å¾„ä¸å¸¦ç»“å°¾ç¬¦å·")
        lines.append("")

        return "\n".join(lines)

    def generate_json(self, structure: List[Dict]) -> str:
        """ç”ŸæˆJSONæ ¼å¼çš„ç›®å½•ç»“æ„

        Args:
            structure: ç›®å½•ç»“æ„æ•°æ®

        Returns:
            JSONæ ¼å¼çš„å­—ç¬¦ä¸²
        """
        output_data = {
            "metadata": {
                "generated_time": datetime.now().isoformat(),
                "title": "é¡¹ç›®ç›®å½•ç»“æ„",
                "statistics": {
                    "total_dirs": self.stats["total_dirs"],
                    "total_files": self.stats["total_files"],
                },
            },
            "structure": structure,
        }
        return json.dumps(output_data, ensure_ascii=False, indent=2)

    def generate_yaml(self, structure: List[Dict]) -> str:
        """ç”ŸæˆYAMLæ ¼å¼çš„ç›®å½•ç»“æ„

        Args:
            structure: ç›®å½•ç»“æ„æ•°æ®

        Returns:
            YAMLæ ¼å¼çš„å­—ç¬¦ä¸²
        """
        output_data = {
            "metadata": {
                "generated_time": datetime.now().isoformat(),
                "title": "é¡¹ç›®ç›®å½•ç»“æ„",
                "statistics": {
                    "total_dirs": self.stats["total_dirs"],
                    "total_files": self.stats["total_files"],
                },
            },
            "structure": structure,
        }
        return yaml.dump(output_data, allow_unicode=True, default_flow_style=False)

    def generate_xml(self, structure: List[Dict]) -> str:
        """ç”ŸæˆXMLæ ¼å¼çš„ç›®å½•ç»“æ„

        Args:
            structure: ç›®å½•ç»“æ„æ•°æ®

        Returns:
            XMLæ ¼å¼çš„å­—ç¬¦ä¸²
        """
        root = ET.Element("project_structure")

        # æ·»åŠ å…ƒæ•°æ®
        metadata = ET.SubElement(root, "metadata")
        ET.SubElement(metadata, "generated_time").text = datetime.now().isoformat()
        ET.SubElement(metadata, "title").text = "é¡¹ç›®ç›®å½•ç»“æ„"

        statistics = ET.SubElement(metadata, "statistics")
        ET.SubElement(statistics, "total_dirs").text = str(self.stats["total_dirs"])
        ET.SubElement(statistics, "total_files").text = str(self.stats["total_files"])

        # æ·»åŠ ç»“æ„æ•°æ®
        structure_elem = ET.SubElement(root, "structure")

        def add_items_to_xml(parent_elem, items):
            """é€’å½’æ·»åŠ é¡¹ç›®åˆ°XML"""
            for item in items:
                item_elem = ET.SubElement(parent_elem, "item")
                item_elem.set("type", item["type"])
                item_elem.set("name", item["name"])
                item_elem.set("path", item["path"])

                if item["type"] == "file" and "size" in item:
                    item_elem.set("size", str(item["size"]))

                if "children" in item and item["children"]:
                    children_elem = ET.SubElement(item_elem, "children")
                    add_items_to_xml(children_elem, item["children"])

        add_items_to_xml(structure_elem, structure)

        # æ ¼å¼åŒ–XML
        rough_string = ET.tostring(root, encoding="unicode")
        reparsed = minidom.parseString(rough_string)
        return reparsed.toprettyxml(indent="  ")

    def save_structure(
        self, structure: List[Dict], output_file: Path, formats: List[str] = None
    ) -> None:
        """ä¿å­˜ç›®å½•ç»“æ„åˆ°æ–‡ä»¶

        Args:
            structure: ç›®å½•ç»“æ„æ•°æ®
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            formats: è¾“å‡ºæ ¼å¼åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ ¼å¼
        """
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_file.parent.mkdir(parents=True, exist_ok=True)

            # è·å–è¾“å‡ºæ ¼å¼
            if formats is None:
                formats = self.config.get("output_formats", ["markdown"])

            generated_files = []

            for format_type in formats:
                if format_type == "markdown":
                    content = self.generate_markdown(structure)
                    file_path = output_file
                elif format_type == "json":
                    content = self.generate_json(structure)
                    file_path = output_file.with_suffix(".json")
                elif format_type == "yaml":
                    content = self.generate_yaml(structure)
                    file_path = output_file.with_suffix(".yaml")
                elif format_type == "xml":
                    content = self.generate_xml(structure)
                    file_path = output_file.with_suffix(".xml")
                else:
                    print(f"âš ï¸  ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {format_type}")
                    continue

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(content)

                generated_files.append(file_path)

            print("\nâœ… ç›®å½•ç»“æ„æ ‡å‡†æ¸…å•å·²ç”Ÿæˆ:")
            for file_path in generated_files:
                print(f"   {file_path}")

        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            raise


async def main_async(formats: List[str] = None):
    """å¼‚æ­¥ä¸»å‡½æ•°

    Args:
        formats: æŒ‡å®šçš„è¾“å‡ºæ ¼å¼åˆ—è¡¨
    """
    try:
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root_str = get_project_root()
        project_root = Path(project_root_str)
        print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")

        # åˆ›å»ºç”Ÿæˆå™¨
        generator = DirectoryStructureGenerator()

        # æ‰«æç›®å½•ç»“æ„ï¼ˆå¸¦æ€§èƒ½ä¼˜åŒ–ï¼‰
        print("ğŸ” æ­£åœ¨æ‰«æç›®å½•ç»“æ„...")
        structure = await generator.scan_directory_with_performance(project_root)

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = project_root / "docs" / "01-è®¾è®¡" / "ç›®å½•ç»“æ„æ ‡å‡†æ¸…å•.md"

        # ä¿å­˜ç»“æ„
        generator.save_structure(structure, output_file, formats)

    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        sys.exit(1)

    print("\n" + "=" * 60)
    print("ç”Ÿæˆå®Œæˆ")
    print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ç”Ÿæˆé¡¹ç›®ç›®å½•ç»“æ„æ ‡å‡†æ¸…å•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""æ”¯æŒçš„è¾“å‡ºæ ¼å¼:
  markdown  - Markdownæ ¼å¼ (é»˜è®¤)
  json      - JSONæ ¼å¼
  yaml      - YAMLæ ¼å¼
  xml       - XMLæ ¼å¼

ç¤ºä¾‹:
  python update_structure.py                    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ ¼å¼
  python update_structure.py -f markdown        # ä»…ç”ŸæˆMarkdownæ ¼å¼
  python update_structure.py -f json yaml       # ç”ŸæˆJSONå’ŒYAMLæ ¼å¼
  python update_structure.py -f all             # ç”Ÿæˆæ‰€æœ‰æ”¯æŒçš„æ ¼å¼""",
    )

    parser.add_argument(
        "-f",
        "--formats",
        nargs="*",
        choices=["markdown", "json", "yaml", "xml", "all"],
        help="æŒ‡å®šè¾“å‡ºæ ¼å¼ (å¯æŒ‡å®šå¤šä¸ª)",
    )

    args = parser.parse_args()

    # å¤„ç†æ ¼å¼å‚æ•°
    formats = None
    if args.formats:
        if "all" in args.formats:
            formats = ["markdown", "json", "yaml", "xml"]
        else:
            formats = args.formats

    asyncio.run(main_async(formats))


if __name__ == "__main__":
    main()
