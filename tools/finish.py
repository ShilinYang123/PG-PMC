#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Work Completion Processing Tool (Python Version)
Purpose: Perform checks, generate reports, and manage end-of-work tasks.
Author: AI9 Tech Lead (Yujun)
Version: 0.1 (Initial Python Structure)
"""

import argparse
import yaml
from pathlib import Path
import fnmatch  # For exclude pattern matching
import importlib
import logging
import os
import re
import shutil  # Needed for potential future move/delete operations
import subprocess
from pathlib import Path
import sys
import time
import urllib.request
import zipfile
from datetime import datetime, timedelta
from pathlib import Path
import json

import yaml  # ç”¨äºåŠ è½½ç›®å½•è§„èŒƒé…ç½®

# å¯¼å…¥é”™è¯¯å¤„ç†æœºåˆ¶
from exceptions import ValidationError, ErrorHandler
from config_loader import get_config

# åˆå§‹åŒ–é”™è¯¯å¤„ç†å™¨
error_handler = ErrorHandler()

# --- Configuration Loading ---
def load_project_config():
    """åŠ è½½é¡¹ç›®é…ç½®"""
    config_path = Path(__file__).parent.parent / "docs" / "03-ç®¡ç†" / "project_config.yaml"
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return None

def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    config = load_project_config()
    if config and config.get('paths', {}).get('root'):
        return Path(config['paths']['root'])
    
    # å¤‡ç”¨æ–¹æ¡ˆ
    return Path(__file__).resolve().parent.parent

# --- MCPå·¥å…·é›†æˆ ---


class MCPToolsManager:
    """MCPå·¥å…·ç®¡ç†å™¨ï¼Œç”¨äºé›†æˆTaskManagerã€memoryç­‰å·¥å…·"""

    def __init__(self, project_root):
        self.project_root = Path(project_root)

        # ä»é…ç½®è¯»å–MCPè·¯å¾„
        try:
            from config_loader import get_mcp_config
            mcp_config = get_mcp_config()

            # æ„å»ºå®Œæ•´è·¯å¾„
            tasks_path = mcp_config['task_manager']['storage_path']
            memory_path = mcp_config['memory']['storage_path']

            self.tasks_file = self.project_root / tasks_path
            self.memory_file = self.project_root / memory_path

            logging.getLogger(__name__).info(f"MCPé…ç½®åŠ è½½æˆåŠŸ - Tasks: {self.tasks_file}, Memory: {self.memory_file}")

        except Exception as e:
            # å›é€€åˆ°é»˜è®¤è·¯å¾„
            logging.getLogger(__name__).warning(f"MCPé…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {e}")
            self.tasks_file = self.project_root / "docs" / "02-å¼€å‘" / "tasks.json"
            self.memory_file = self.project_root / "docs" / "02-å¼€å‘" / "memory.json"

        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        self.tasks_file.parent.mkdir(parents=True, exist_ok=True)
        self.memory_file.parent.mkdir(parents=True, exist_ok=True)

    def create_approval_task(
            self,
            title,
            description,
            operation_type="approval",
            priority="normal"):
        """åˆ›å»ºéœ€è¦äººå·¥å®¡æ‰¹çš„ä»»åŠ¡"""
        try:
            # æ¨¡æ‹ŸTaskManageråˆ›å»ºå®¡æ‰¹ä»»åŠ¡çš„åŠŸèƒ½
            task_data = {
                "id": f"approval_{int(time.time())}",
                "title": title,
                "description": description,
                "type": operation_type,
                "priority": priority,
                "status": "pending_approval",
                "created_at": datetime.now().isoformat(),
                "requires_human_approval": True
            }

            # è¯»å–ç°æœ‰ä»»åŠ¡
            tasks = []
            if self.tasks_file.exists():
                try:
                    with open(self.tasks_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        # ç¡®ä¿tasksæ˜¯åˆ—è¡¨æ ¼å¼
                        if isinstance(data, list):
                            tasks = data
                        else:
                            logger.warning("Tasksæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œé‡æ–°åˆå§‹åŒ–ä¸ºåˆ—è¡¨")
                            tasks = []
                except (json.JSONDecodeError, ValueError) as e:
                    logger.warning(f"Tasksæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œé‡æ–°åˆå§‹åŒ–: {e}")
                    tasks = []

            # æ·»åŠ æ–°ä»»åŠ¡
            tasks.append(task_data)

            # ä¿å­˜ä»»åŠ¡
            with open(self.tasks_file, 'w', encoding='utf-8') as f:
                json.dump(tasks, f, ensure_ascii=False, indent=2)

            logger.info(f"å·²åˆ›å»ºå®¡æ‰¹ä»»åŠ¡: {title} (ID: {task_data['id']})")
            return task_data['id']

        except Exception as e:
            logger.error(f"åˆ›å»ºå®¡æ‰¹ä»»åŠ¡å¤±è´¥: {e}")
            return None

    def store_memory(self, key, content, category="general"):
        """å­˜å‚¨é‡è¦ä¿¡æ¯åˆ°memory"""
        try:
            # è¯»å–ç°æœ‰è®°å¿†
            memory_data = {}
            if self.memory_file.exists():
                try:
                    with open(self.memory_file, 'r', encoding='utf-8') as f:
                        memory_data = json.load(f)
                except (json.JSONDecodeError, ValueError) as e:
                    logger.warning(f"Memoryæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå°†é‡æ–°åˆ›å»º: {e}")
                    memory_data = {}

            # æŒ‰ç±»åˆ«ç»„ç»‡è®°å¿†
            if category not in memory_data:
                memory_data[category] = {}

            memory_data[category][key] = {
                "content": content,
                "timestamp": datetime.now().isoformat(),
                "updated_by": "finish.py"
            }

            # ä¿å­˜è®°å¿†
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump(memory_data, f, ensure_ascii=False, indent=2)

            logger.info(f"å·²å­˜å‚¨è®°å¿†: {category}/{key}")
            return True

        except Exception as e:
            logger.error(f"å­˜å‚¨è®°å¿†å¤±è´¥: {e}")
            return False

    def get_memory(self, key, category="general"):
        """ä»memoryè·å–ä¿¡æ¯"""
        try:
            if not self.memory_file.exists():
                return None

            try:
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    memory_data = json.load(f)
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"Memoryæ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
                return None

            if category in memory_data and key in memory_data[category]:
                return memory_data[category][key]["content"]

            return None

        except Exception as e:
            logger.error(f"è·å–è®°å¿†å¤±è´¥: {e}")
            return None

    def wait_for_approval(self, task_id, timeout_minutes=60):
        """ç­‰å¾…äººå·¥å®¡æ‰¹ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        logger.warning(f"ä»»åŠ¡ {task_id} éœ€è¦äººå·¥å®¡æ‰¹ï¼Œè¯·æ£€æŸ¥ {self.tasks_file}")
        logger.warning("åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œæ­¤å¤„åº”æš‚åœæ‰§è¡Œç­‰å¾…äººå·¥å®¡æ‰¹")
        logger.warning("å½“å‰ä¸ºæ¼”ç¤ºæ¨¡å¼ï¼Œå°†ç»§ç»­æ‰§è¡Œ")
        return True  # æ¼”ç¤ºæ¨¡å¼ä¸‹ç›´æ¥è¿”å›True

    def sequential_thinking(
            self,
            context,
            decision_points,
            reasoning_type="analysis"):
        """Sequential thinking for critical decision points"""
        try:
            thinking_result = {
                "context": context,
                "reasoning_type": reasoning_type,
                "decision_points": decision_points,
                "timestamp": datetime.now().isoformat(),
                "conclusions": []
            }

            logger.info(f"å¼€å§‹é€»è¾‘æ¨ç†: {context}")

            # é€æ­¥åˆ†ææ¯ä¸ªå†³ç­–ç‚¹
            for i, point in enumerate(decision_points, 1):
                logger.info(f"åˆ†æå†³ç­–ç‚¹ {i}: {point['question']}")

                # æ¨¡æ‹Ÿé€»è¾‘æ¨ç†è¿‡ç¨‹
                analysis = {
                    "step": i,
                    "question": point['question'],
                    "factors": point.get('factors', []),
                    "risks": point.get('risks', []),
                    "benefits": point.get('benefits', []),
                    "recommendation": point.get('recommendation', 'éœ€è¦è¿›ä¸€æ­¥åˆ†æ')
                }

                thinking_result["conclusions"].append(analysis)
                logger.info(f"å†³ç­–ç‚¹ {i} åˆ†æå®Œæˆ: {analysis['recommendation']}")

            # å­˜å‚¨æ€è€ƒç»“æœåˆ°memory
            self.store_memory(
                f"thinking_{reasoning_type}",
                thinking_result,
                "sequential_thinking")

            logger.info(f"é€»è¾‘æ¨ç†å®Œæˆï¼Œå…±åˆ†æ {len(decision_points)} ä¸ªå†³ç­–ç‚¹")
            return thinking_result

        except Exception as e:
            logger.error(f"Sequential thinking å¤±è´¥: {e}")
            return None

    def get_thinking_history(self, reasoning_type=None):
        """è·å–å†å²æ€è€ƒè®°å½•"""
        try:
            if not self.memory_file.exists():
                return []

            try:
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    memory_data = json.load(f)
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"Memoryæ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
                return []

            thinking_data = memory_data.get("sequential_thinking", {})

            if reasoning_type:
                return thinking_data.get(f"thinking_{reasoning_type}", {})
            else:
                return thinking_data

        except Exception as e:
            logger.error(f"è·å–æ€è€ƒå†å²å¤±è´¥: {e}")
            return []

# --- å…¨æ–°å‚æ•°è§£æç³»ç»Ÿ ---


def init_arg_parser():
    """åˆå§‹åŒ–å¹¶è¿”å›é…ç½®å¥½çš„å‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="AI9é¡¹ç›®å·¥ä½œæµç®¡ç†ç³»ç»Ÿ",
        formatter_class=argparse.RawTextHelpFormatter,
        add_help=False,
        epilog="ç¤ºä¾‹:\n  python finish.py --daily --backup-dir ./backups\n"
    )

    # ä¸»æ“ä½œæ¨¡å¼å‚æ•°ç»„ (å¿…é¡»ä¸”å”¯ä¸€)
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument("--daily",
                            action="store_true",
                            help="æ—¥å¸¸å¤‡ä»½æ¨¡å¼ï¼ˆåŒ…å«è‡ªåŠ¨å¤‡ä»½å’Œæ—¥æŠ¥ç”Ÿæˆï¼‰")
    mode_group.add_argument("--release",
                            action="store_true",
                            help="å‘å¸ƒæ¨¡å¼ï¼ˆå®Œæ•´æ ¡éªŒå¹¶ç”Ÿæˆå‘å¸ƒåŒ…ï¼‰")
    mode_group.add_argument("--backup-only",
                            action="store_true",
                            help="å¿«é€Ÿå¤‡ä»½æ¨¡å¼ï¼ˆä»…æ‰§è¡Œé¡¹ç›®å¤‡ä»½ï¼Œè·³è¿‡å…¶ä»–æ£€æŸ¥ï¼‰")
    mode_group.add_argument("--init-config",
                            action="store_true",
                            help="åˆå§‹åŒ–é…ç½®æ–‡ä»¶ï¼ˆç”Ÿæˆé»˜è®¤é…ç½®ï¼‰")
    mode_group.add_argument("--self-check",
                            action="store_true",
                            help="æ¯æ—¥å·¥ä½œè‡ªæ£€æ¨¡å¼ï¼ˆæ ¹æ®åŸºæœ¬è§„åˆ™æ‰§è¡Œæ£€æŸ¥å¹¶ç”ŸæˆæŠ¥å‘Šï¼‰")

    # è·¯å¾„å‚æ•°ç»„
    path_group = parser.add_argument_group('è·¯å¾„å‚æ•°')
    path_group.add_argument("--backup-dir",
                            metavar="PATH",
                            help="æŒ‡å®šè‡ªå®šä¹‰å¤‡ä»½è·¯å¾„\n(ä¼˜å…ˆäºé…ç½®æ–‡ä»¶è®¾ç½®)")
    path_group.add_argument("--config",
                            metavar="FILE",
                            help="ä½¿ç”¨æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„")

    # åŠŸèƒ½æ§åˆ¶å‚æ•°ç»„
    control_group = parser.add_argument_group('åŠŸèƒ½æ§åˆ¶å‚æ•°')
    control_group.add_argument("--no-backup",
                               action="store_true",
                               help="è·³è¿‡é¡¹ç›®å¤‡ä»½æ­¥éª¤")
    control_group.add_argument("--no-quality-check",
                               action="store_true",
                               help="è·³è¿‡ä»£ç è´¨é‡æ£€æŸ¥")
    control_group.add_argument("--auto-cleanup",
                               action="store_true",
                               help="è‡ªåŠ¨æ¸…ç†è¿è§„ç›®å½•åˆ°é¡¹ç›®å¤–éƒ¨")

    # è°ƒè¯•å‚æ•°ç»„
    debug_group = parser.add_argument_group('è°ƒè¯•å‚æ•°')
    debug_group.add_argument("--verbose",
                             action="store_true",
                             help="å¯ç”¨è¯¦ç»†è°ƒè¯•è¾“å‡º")
    debug_group.add_argument("-h", "--help",
                             action="help",
                             help="æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯å¹¶é€€å‡º")

    return parser


def validate_args(args):
    """å‚æ•°é€»è¾‘æ ¡éªŒ"""
    if args.backup_dir and not (args.daily or args.release):
        raise ValueError("--backup-dir å¿…é¡»ä¸ --daily/--release æ¨¡å¼é…åˆä½¿ç”¨")
    if args.config and not os.path.exists(args.config):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
    if args.backup_dir and not os.path.isabs(args.backup_dir):
        args.backup_dir = os.path.abspath(args.backup_dir)


# åˆå§‹åŒ–å‚æ•°ç³»ç»Ÿ
parser = init_arg_parser()
args = parser.parse_args()

try:
    validate_args(args)
except (ValueError, FileNotFoundError) as e:
    parser.error(str(e))

# --- Constants and Global Setup ---
NL = "\\n"  # Using literal newline for Markdown compatibility in Python strings
PROJECT_ROOT = get_project_root()

# åˆå§‹åŒ–MCPå·¥å…·ç®¡ç†å™¨
mcp_tools = MCPToolsManager(PROJECT_ROOT)

# --- ç»Ÿä¸€è·¯å¾„é…ç½®å˜é‡ ---
# é¡¹ç›®æ ‡å‡†è·¯å¾„é…ç½®
config = load_project_config()
if config and config.get('paths'):
    STANDARD_BACKUP_DIR = Path(config['paths'].get('backup_dir', PROJECT_ROOT / "bak"))
    STANDARD_LOGS_DIR = Path(config['paths'].get('logs_dir', PROJECT_ROOT / "logs"))
else:
    STANDARD_BACKUP_DIR = PROJECT_ROOT / "bak"  # æ ‡å‡†å¤‡ä»½ç›®å½•
    STANDARD_LOGS_DIR = PROJECT_ROOT / "logs"  # æ ‡å‡†æ—¥å¿—ç›®å½•
STANDARD_CLEANUP_DIR = STANDARD_BACKUP_DIR / "å¾…æ¸…ç†èµ„æ–™"  # æ ‡å‡†æ¸…ç†ç›®å½•

# åŸºç¡€ç›®å½•é…ç½®
LOG_DIR = STANDARD_LOGS_DIR / "å·¥ä½œè®°å½•"
# ä»é…ç½®æ–‡ä»¶è·å–æŠ¥å‘Šç›®å½•
config = get_config()
report_dir_config = config.get('structure_check', {}).get('report_dir', 'logs/æ£€æŸ¥æŠ¥å‘Š')
REPORT_DIR = PROJECT_ROOT / report_dir_config
TIMESTAMP = datetime.now().strftime("%Y%m%d-%H%M%S")
LOG_FILE = LOG_DIR / f"finish_py_{TIMESTAMP}.log"
REPORT_FILE = LOG_DIR / f"finish_report_py_{TIMESTAMP}.md"

# BACKUP_DIR å…ˆå ä½ï¼Œéšåæ ¹æ®è§„èŒƒé…ç½®åŠ¨æ€å®šä¹‰
BACKUP_DIR = None
BACKUP_LOG_DIR = STANDARD_LOGS_DIR / "å…¶ä»–æ—¥å¿—"  # Directory for the backup log file
BACKUP_LOG_FILE = BACKUP_LOG_DIR / "å¤‡ä»½ç³»ç»Ÿæ—¥å¿—.md"
BACKUP_RETENTION_COUNT = 60  # Keep last 60 backups

# Script-level counters
issues_found = 0
warnings_found = 0

# Debug: Print directory paths
print(f"DEBUG: PROJECT_ROOT = {PROJECT_ROOT}")
print(f"DEBUG: STANDARD_LOGS_DIR = {STANDARD_LOGS_DIR}")
print(f"DEBUG: LOG_DIR = {LOG_DIR}")
print(f"DEBUG: REPORT_DIR = {REPORT_DIR}")

# Ensure directories exist
LOG_DIR.mkdir(parents=True, exist_ok=True)
REPORT_DIR.mkdir(parents=True, exist_ok=True)  # Ensure parent dirs exist too

# --- Logging Setup ---
log_formatter = logging.Formatter(
    "%(asctime)s [%(levelname)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# File Handler
file_handler = logging.FileHandler(LOG_FILE, encoding="utf-8")
file_handler.setFormatter(log_formatter)
logger.addHandler(file_handler)

# Console Handler (optional, for immediate feedback)
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(log_formatter)
# Map log levels to colors (requires a library like 'colorlog' or manual ANSI codes)
# For simplicity, we'll use basic console output for now
# TODO: Add colorized console output if needed
logger.addHandler(console_handler)

# FIRST_EDIT: åŠ è½½ç›®å½•è§„èŒƒé…ç½®ï¼Œç”¨äº invoke_directory_check å’Œ agent_verify_structure


def get_daily_git_commits():
    """è·å–å½“å¤©çš„ Git æäº¤è®°å½•"""
    try:
        # è·å–ä»Šå¤©çš„æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD
        today_date = datetime.now().strftime("%Y-%m-%d")
        # æ„å»º git log å‘½ä»¤ï¼Œè·å–ä»Šå¤©çš„æäº¤è®°å½•ï¼Œæ ¼å¼åŒ–è¾“å‡º
        # --after å’Œ --before ç”¨äºæŒ‡å®šæ—¥æœŸèŒƒå›´ï¼Œç¡®ä¿åªè·å–å½“å¤©çš„è®°å½•
        # --pretty=format:'- %h %s (%an)' æŒ‡å®šè¾“å‡ºæ ¼å¼ï¼šçŸ­å“ˆå¸Œ ä½œè€… æäº¤ä¿¡æ¯
        # --no-merges æ’é™¤åˆå¹¶æäº¤
        cmd = [
            "git", "log", f"--after={today_date} 00:00:00",
            f"--before={today_date} 23:59:59",
            "--pretty=format:- %h %s (%an)", "--no-merges"
        ]
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True,
            encoding='utf-8',
            cwd=PROJECT_ROOT)
        commits = result.stdout.strip()
        if commits:
            logger.info("æˆåŠŸè·å–å½“æ—¥ Git æäº¤è®°å½•ã€‚")
            return f"### å½“æ—¥ Git æäº¤è®°å½•\n\n{commits}\n"
        else:
            logger.info("å½“æ—¥æ—  Git æäº¤è®°å½•ã€‚")
            return "### å½“æ—¥ Git æäº¤è®°å½•\n\nå½“æ—¥æ— æäº¤è®°å½•ã€‚\n"
    except subprocess.CalledProcessError as e:
        logger.error(f"è·å– Git æäº¤è®°å½•å¤±è´¥: {e.stderr}")
        return "### å½“æ—¥ Git æäº¤è®°å½•\n\nè·å–æäº¤è®°å½•å¤±è´¥ã€‚\n"
    except FileNotFoundError:
        logger.error("Git å‘½ä»¤æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿ Git å·²å®‰è£…å¹¶é…ç½®åœ¨ç³»ç»Ÿè·¯å¾„ä¸­ã€‚")
        return "### å½“æ—¥ Git æäº¤è®°å½•\n\nGit å‘½ä»¤æœªæ‰¾åˆ°ã€‚\n"


if args.init_config:  # æ–°å¢åˆå§‹åŒ–é…ç½®æ–‡ä»¶é€»è¾‘
    default_spec = {
        "backup_dir": str(STANDARD_BACKUP_DIR),
        "required_dirs": ["src", "logs"]
    }
    # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›äº†çš„è¯
    config_path = Path(args.config) if args.config else PROJECT_ROOT / \
        "docs" / "03-ç®¡ç†" / "project_config.yaml"
    # è·å–ç”¨æˆ·æŒ‡å®šçš„å¤‡ä»½ç›®å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    user_backup_dir = args.backup_dir or None
    if user_backup_dir:
        default_spec["backup_dir"] = str(Path(user_backup_dir).resolve())

    # ç¡®ä¿é…ç½®æ–‡ä»¶çˆ¶ç›®å½•å­˜åœ¨
    config_path.parent.mkdir(parents=True, exist_ok=True)

    # å†™å…¥é…ç½®æ–‡ä»¶
    with open(config_path, "w", encoding="utf-8") as f:
        yaml.safe_dump(default_spec, f, allow_unicode=True, sort_keys=False)

    logger.info(f"åˆå§‹é…ç½®æ–‡ä»¶å·²åˆ›å»ºï¼š{config_path}")
    logger.debug(f"é…ç½®æ–‡ä»¶å†…å®¹ï¼š{default_spec}")
    sys.exit(0)

try:
    # ä¼˜å…ˆä½¿ç”¨--configå‚æ•°æŒ‡å®šçš„é…ç½®æ–‡ä»¶
    config_path = Path(args.config) if args.config else \
        PROJECT_ROOT / "docs" / "03-ç®¡ç†" / "project_config.yaml"
    with open(config_path, "r", encoding="utf-8") as f:
        full_config = yaml.safe_load(f)
        # ä»project_config.yamlçš„pathséƒ¨åˆ†æå–directory_specæ ¼å¼çš„é…ç½®
        dir_spec = {
            "required_dirs": full_config.get('paths', {}).get('required_dirs', []),
            "required_files": full_config.get('paths', {}).get('required_files', [])
        }
    logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶ï¼š{config_path}")
except FileNotFoundError:
    logger.error(
        "Directory spec config not found. Use --init-config to create one.")
    sys.exit(1)
except Exception as e:
    logger.error(f"Failed to load directory specification: {e}")
    sys.exit(1)

# --- Self-Check Mode Logic ---


def run_self_check():
    """æ‰§è¡Œæ¯æ—¥å·¥ä½œè‡ªæ£€æµç¨‹ï¼Œé›†æˆTaskManagerå’Œmemoryå·¥å…·"""
    report_content = "# æ¯æ—¥å·¥ä½œè‡ªæ£€æŠ¥å‘Š\n\n"
    logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥å·¥ä½œè‡ªæ£€æµç¨‹...")

    # å­˜å‚¨è‡ªæ£€å¼€å§‹ä¿¡æ¯åˆ°memory
    mcp_tools.store_memory("self_check_start", {
        "timestamp": TIMESTAMP,
        "status": "started",
        "report_type": "daily_self_check"
    })

    # 1. è·å–å½“æ—¥ Git æäº¤è®°å½•
    report_content += get_daily_git_commits()

    # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºå®¡æ‰¹ä»»åŠ¡
    critical_issues = []

    # 3. ä»£ç è´¨é‡æ£€æŸ¥ (Flake8, ESLint)
    logger.info("æ‰§è¡Œä»£ç è´¨é‡æ£€æŸ¥...")
    try:
        # è¿è¡Œflake8æ£€æŸ¥ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶æ’é™¤.venvç­‰ç›®å½•
        config_file = PROJECT_ROOT / "project" / "setup.cfg"
        flake8_cmd = ["flake8", "--max-line-length=88"]
        if config_file.exists():
            flake8_cmd.extend(["--config", str(config_file)])
        flake8_cmd.append(".")
        
        flake8_result = subprocess.run(
            flake8_cmd,
            capture_output=True, text=True, cwd=PROJECT_ROOT
        )
        if flake8_result.returncode != 0:
            report_content += "\n### ä»£ç è´¨é‡æ£€æŸ¥\n\n"
            report_content += "**Flake8æ£€æŸ¥å‘ç°é—®é¢˜:**\n```\n"
            report_content += flake8_result.stdout
            report_content += "```\n"
            critical_issues.append("ä»£ç è´¨é‡é—®é¢˜")
        else:
            report_content += "\n### ä»£ç è´¨é‡æ£€æŸ¥\n\nâœ… Flake8æ£€æŸ¥é€šè¿‡\n"
    except FileNotFoundError:
        report_content += "\n### ä»£ç è´¨é‡æ£€æŸ¥\n\nâš ï¸ Flake8æœªå®‰è£…æˆ–ä¸å¯ç”¨\n"

    # 4. è‡ªåŠ¨åŒ–æµ‹è¯•æ£€æŸ¥
    logger.info("æ£€æŸ¥æµ‹è¯•æ–‡ä»¶...")
    test_files = list(PROJECT_ROOT.glob("**/test_*.py")) + \
        list(PROJECT_ROOT.glob("**/*_test.py"))
    if test_files:
        report_content += "\n### æµ‹è¯•æ–‡ä»¶æ£€æŸ¥\n\n"
        report_content += f"å‘ç° {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶:\n"
        for test_file in test_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            report_content += f"- {test_file.relative_to(PROJECT_ROOT)}\n"
        if len(test_files) > 5:
            report_content += f"- ... è¿˜æœ‰ {len(test_files) - 5} ä¸ªæ–‡ä»¶\n"
    else:
        report_content += "\n### æµ‹è¯•æ–‡ä»¶æ£€æŸ¥\n\nâš ï¸ æœªå‘ç°æµ‹è¯•æ–‡ä»¶\n"
        critical_issues.append("ç¼ºå°‘æµ‹è¯•æ–‡ä»¶")

    # 5. æ–‡æ¡£æ›´æ–°æ£€æŸ¥
    logger.info("æ£€æŸ¥æ–‡æ¡£æ›´æ–°...")
    docs_dir = PROJECT_ROOT / "docs"
    if docs_dir.exists():
        recent_docs = []
        for doc_file in docs_dir.rglob("*.md"):
            if doc_file.stat().st_mtime > (datetime.now() - timedelta(days=7)).timestamp():
                recent_docs.append(doc_file)

        report_content += "\n### æ–‡æ¡£æ›´æ–°æ£€æŸ¥\n\n"
        if recent_docs:
            report_content += f"è¿‘7å¤©å†…æ›´æ–°çš„æ–‡æ¡£ ({len(recent_docs)} ä¸ª):\n"
            for doc in recent_docs[:3]:
                report_content += f"- {doc.relative_to(PROJECT_ROOT)}\n"
        else:
            report_content += "âš ï¸ è¿‘7å¤©å†…æ— æ–‡æ¡£æ›´æ–°\n"

    # 6. é¡¹ç›®è§„èŒƒç¬¦åˆæ€§æ£€æŸ¥
    logger.info("æ£€æŸ¥é¡¹ç›®è§„èŒƒç¬¦åˆæ€§...")
    required_files = ["README.md", "requirements.txt", ".gitignore"]
    missing_files = []
    for req_file in required_files:
        if not (PROJECT_ROOT / req_file).exists():
            missing_files.append(req_file)

    if missing_files:
        report_content += "\n### é¡¹ç›®è§„èŒƒæ£€æŸ¥\n\n"
        report_content += "âš ï¸ ç¼ºå°‘å¿…è¦æ–‡ä»¶:\n"
        for missing in missing_files:
            report_content += f"- {missing}\n"
        critical_issues.append("ç¼ºå°‘å¿…è¦æ–‡ä»¶")
    else:
        report_content += "\n### é¡¹ç›®è§„èŒƒæ£€æŸ¥\n\nâœ… å¿…è¦æ–‡ä»¶å®Œæ•´\n"

    # 7. Sequential thinking for critical issues analysis
    if critical_issues:
        logger.info(f"å‘ç° {len(critical_issues)} ä¸ªå…³é”®é—®é¢˜ï¼Œå¼€å§‹é€»è¾‘æ¨ç†åˆ†æ...")

        # æ„å»ºå†³ç­–ç‚¹
        decision_points = []
        for issue in critical_issues:
            if "ä»£ç è´¨é‡" in issue:
                decision_points.append({
                    "question": "å¦‚ä½•å¤„ç†ä»£ç è´¨é‡é—®é¢˜ï¼Ÿ",
                    "factors": ["ä»£ç å¯ç»´æŠ¤æ€§", "å›¢é˜Ÿå¼€å‘æ•ˆç‡", "é¡¹ç›®ç¨³å®šæ€§"],
                    "risks": ["æŠ€æœ¯å€ºåŠ¡ç´¯ç§¯", "bugå¢åŠ ", "å¼€å‘æ•ˆç‡ä¸‹é™"],
                    "benefits": ["æé«˜ä»£ç è´¨é‡", "å‡å°‘ç»´æŠ¤æˆæœ¬", "æå‡å›¢é˜Ÿåä½œ"],
                    "recommendation": "ç«‹å³ä¿®å¤ä»£ç è´¨é‡é—®é¢˜ï¼Œå»ºç«‹ä»£ç å®¡æŸ¥æµç¨‹"
                })
            elif "æµ‹è¯•æ–‡ä»¶" in issue:
                decision_points.append({
                    "question": "å¦‚ä½•è¡¥å……æµ‹è¯•è¦†ç›–ï¼Ÿ",
                    "factors": ["ä»£ç è¦†ç›–ç‡", "æµ‹è¯•ç­–ç•¥", "å¼€å‘æ—¶é—´"],
                    "risks": ["åŠŸèƒ½å›å½’", "è´¨é‡ä¸å¯æ§", "éƒ¨ç½²é£é™©"],
                    "benefits": ["æé«˜ä»£ç è´¨é‡", "å‡å°‘bug", "å¢å¼ºä¿¡å¿ƒ"],
                    "recommendation": "ä¼˜å…ˆä¸ºæ ¸å¿ƒåŠŸèƒ½ç¼–å†™å•å…ƒæµ‹è¯•"
                })
            elif "å¿…è¦æ–‡ä»¶" in issue:
                decision_points.append({
                    "question": "å¦‚ä½•å®Œå–„é¡¹ç›®ç»“æ„ï¼Ÿ",
                    "factors": ["é¡¹ç›®è§„èŒƒ", "å›¢é˜Ÿåä½œ", "éƒ¨ç½²è¦æ±‚"],
                    "risks": ["é¡¹ç›®æ··ä¹±", "éƒ¨ç½²å¤±è´¥", "åä½œå›°éš¾"],
                    "benefits": ["è§„èŒƒåŒ–ç®¡ç†", "æå‡ä¸“ä¸šåº¦", "ä¾¿äºç»´æŠ¤"],
                    "recommendation": "ç«‹å³åˆ›å»ºç¼ºå¤±çš„å¿…è¦æ–‡ä»¶"
                })

        # æ‰§è¡Œé€»è¾‘æ¨ç†
        thinking_result = mcp_tools.sequential_thinking(
            context="æ¯æ—¥è‡ªæ£€å‘ç°å…³é”®é—®é¢˜çš„å¤„ç†å†³ç­–",
            decision_points=decision_points,
            reasoning_type="self_check_analysis"
        )

        if thinking_result:
            report_content += "\n### é€»è¾‘æ¨ç†åˆ†æ\n\n"
            for conclusion in thinking_result["conclusions"]:
                report_content += f"**å†³ç­–ç‚¹ {
                    conclusion['step']}**: {
                    conclusion['question']}\n"
                report_content += f"- æ¨èæ–¹æ¡ˆ: {conclusion['recommendation']}\n\n"

        # åˆ›å»ºå®¡æ‰¹ä»»åŠ¡
        task_description = f"è‡ªæ£€å‘ç°å…³é”®é—®é¢˜éœ€è¦å¤„ç†ï¼š{', '.join(critical_issues)}"
        task_id = mcp_tools.create_approval_task(
            title="æ¯æ—¥è‡ªæ£€å…³é”®é—®é¢˜å¤„ç†",
            description=task_description,
            priority="high"
        )
        report_content += f"\n### å®¡æ‰¹ä»»åŠ¡\n\nâš ï¸ å·²åˆ›å»ºå®¡æ‰¹ä»»åŠ¡ #{task_id}ï¼š{task_description}\n"

        # å­˜å‚¨å®¡æ‰¹ä»»åŠ¡ä¿¡æ¯åˆ°memory
        mcp_tools.store_memory(f"approval_task_{task_id}", {
            "task_id": task_id,
            "issues": critical_issues,
            "thinking_result": thinking_result,
            "created_at": TIMESTAMP,
            "status": "pending"
        })
    else:
        report_content += "\n### æ£€æŸ¥ç»“æœ\n\nâœ… æœªå‘ç°å…³é”®é—®é¢˜\n"

    # 8. å­˜å‚¨è‡ªæ£€ç»“æœåˆ°memory
    self_check_result = {
        "timestamp": TIMESTAMP,
        "status": "completed",
        "critical_issues_count": len(critical_issues),
        "critical_issues": critical_issues,
        "has_approval_task": len(critical_issues) > 0
    }
    mcp_tools.store_memory("self_check_result", self_check_result)

    # å®šä¹‰è‡ªæ£€æŠ¥å‘Šçš„ç›®å½•å’Œæ–‡ä»¶å
    # ä½¿ç”¨å¸¸é‡ä¸­å®šä¹‰çš„ REPORT_DIR
    # REPORT_DIR å·²ç»åœ¨å¸¸é‡åŒºä»é…ç½®æ–‡ä»¶åŠ¨æ€è·å–
    os.makedirs(REPORT_DIR, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
    # TIMESTAMP ä¹Ÿå·²åœ¨å¸¸é‡åŒºå®šä¹‰
    report_filename = f"æ¯æ—¥è‡ªæ£€æŠ¥å‘Š_{TIMESTAMP}.md"
    report_path = REPORT_DIR / report_filename

    try:
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        logger.info(f"è‡ªæ£€æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

        # å°†æŠ¥å‘Šå¤åˆ¶åˆ° Git ä»“åº“å¹¶æäº¤
        git_repo_path = str(BACKUP_DIR / "github_repo")  # ä½¿ç”¨æ­£ç¡®çš„Gitä»“åº“è·¯å¾„
        git_report_filename = os.path.basename(
            report_path)  # ä½¿ç”¨ report_path ä¸­çš„æ–‡ä»¶åéƒ¨åˆ†

        # ç¡®ä¿Gitä»“åº“ä¸­å­˜åœ¨ç¬¦åˆGitHubä»“åº“ç»“æ„è§„èŒƒçš„æ–‡ä»¶å¤¹
        github_dirs = ["docs", "project", "tools"]
        for dir_name in github_dirs:
            dir_path = os.path.join(git_repo_path, dir_name)
            if not os.path.exists(dir_path):
                os.makedirs(dir_path, exist_ok=True)
                logger.info(f"åœ¨Gitä»“åº“ä¸­åˆ›å»ºäº†ç›®å½•ï¼š{dir_path}")

        # å°†è‡ªæ£€æŠ¥å‘Šæ”¾åœ¨docsç›®å½•ä¸‹
        git_report_path = os.path.join(
            git_repo_path, "docs", git_report_filename)
        try:
            shutil.copy2(report_path, git_report_path)
            logger.info(f"è‡ªæ£€æŠ¥å‘Šå·²å¤åˆ¶åˆ° Git ä»“åº“ï¼š{git_report_path}")

            commit_message = f"è‡ªåŠ¨æäº¤è‡ªæ£€æŠ¥å‘Šï¼š{git_report_filename}"
            # ç¡®ä¿ subprocess.run è°ƒç”¨æ—¶ä¼ é€’ encoding å‚æ•°
            subprocess.run(["git",
                            "add",
                            git_report_path],
                           cwd=git_repo_path,
                           check=True,
                           encoding='utf-8',
                           errors='replace')
            subprocess.run(["git",
                            "commit",
                            "-m",
                            commit_message],
                           cwd=git_repo_path,
                           check=True,
                           encoding='utf-8',
                           errors='replace')
            logger.info(f"è‡ªæ£€æŠ¥å‘Šå·²æäº¤åˆ° Git ä»“åº“ï¼Œæäº¤ä¿¡æ¯ï¼š{commit_message}")
            # æ›´æ–°æŠ¥å‘Šå†…å®¹ï¼Œè¿½åŠ æäº¤æˆåŠŸä¿¡æ¯
            with open(report_path, "a", encoding="utf-8") as f:
                f.write(f"\n\nè‡ªæ£€æŠ¥å‘Šå·²è‡ªåŠ¨æäº¤åˆ° Git ä»“åº“ï¼š`{git_repo_path}`\n")

        except Exception as e:
            logger.error(f"å¤åˆ¶æˆ–æäº¤è‡ªæ£€æŠ¥å‘Šåˆ° Git ä»“åº“å¤±è´¥ï¼š{e}")
            # æ›´æ–°æŠ¥å‘Šå†…å®¹ï¼Œè¿½åŠ æäº¤å¤±è´¥ä¿¡æ¯
            with open(report_path, "a", encoding="utf-8") as f:
                f.write(f"\n\nå¤åˆ¶æˆ–æäº¤è‡ªæ£€æŠ¥å‘Šåˆ° Git ä»“åº“å¤±è´¥ï¼š{e}\n")

    except IOError as e:
        logger.error(f"æ— æ³•å†™å…¥è‡ªæ£€æŠ¥å‘Š: {e}")

    logger.info("æ¯æ—¥å·¥ä½œè‡ªæ£€æµç¨‹ç»“æŸã€‚")

    # è¿”å›è‡ªæ£€ç»“æœä¾›å…¶ä»–å‡½æ•°ä½¿ç”¨
    return self_check_result


# DYNAMIC BACKUP_DIR è®¾ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼‰
# å‚æ•°ä¼˜å…ˆçº§ï¼šCLI > é…ç½®æ–‡ä»¶ > é»˜è®¤å€¼
if args.backup_dir:
    BACKUP_DIR = Path(args.backup_dir)
else:
    # é»˜è®¤ä½¿ç”¨é¡¹ç›®è§„èŒƒè¦æ±‚çš„å¤‡ä»½ç›®å½•
    BACKUP_DIR = STANDARD_BACKUP_DIR
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æœ‰æŒ‡å®šï¼Œåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶çš„è®¾ç½®
    if "backup_dir" in dir_spec:
        config_backup_dir = dir_spec.get("backup_dir")
        if not Path(config_backup_dir).is_absolute():
            # ç›¸å¯¹è·¯å¾„ï¼Œç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„çˆ¶ç›®å½•
            BACKUP_DIR = PROJECT_ROOT.parent / config_backup_dir
        else:
            BACKUP_DIR = Path(config_backup_dir)

logger.info(f"å½“å‰å¤‡ä»½ç›®å½•ï¼š{BACKUP_DIR}")
if not BACKUP_DIR.exists():
    try:
        BACKUP_DIR.mkdir(parents=True, exist_ok=True)
        logger.info(f"å·²åˆ›å»ºå¤‡ä»½ç›®å½•ï¼š{BACKUP_DIR}")
    except Exception as e:
        logger.error(f"æ— æ³•åˆ›å»ºå¤‡ä»½ç›®å½• {BACKUP_DIR}: {e}")
        # å›é€€åˆ°é¡¹ç›®å†…çš„å¤‡ä»½ç›®å½•
        BACKUP_DIR = PROJECT_ROOT / "backups"
        BACKUP_DIR.mkdir(parents=True, exist_ok=True)
        logger.warning(f"ä½¿ç”¨å›é€€å¤‡ä»½ç›®å½•ï¼š{BACKUP_DIR}")

# --- Main Execution Logic ---
if __name__ == "__main__":
    if args.self_check:
        run_self_check()
    elif args.daily:
        # ... (ä¿ç•™åŸæœ‰ daily æ¨¡å¼é€»è¾‘)
        logger.info("æ‰§è¡Œæ—¥å¸¸å¤‡ä»½æ¨¡å¼...")
    elif args.release:
        # ... (ä¿ç•™åŸæœ‰ release æ¨¡å¼é€»è¾‘)
        logger.info("æ‰§è¡Œå‘å¸ƒæ¨¡å¼...")
    elif args.backup_only:
        # ... (ä¿ç•™åŸæœ‰ backup_only æ¨¡å¼é€»è¾‘)
        logger.info("æ‰§è¡Œå¿«é€Ÿå¤‡ä»½æ¨¡å¼...")
    # init-config æ¨¡å¼åœ¨å‚æ•°è§£æåå·²å¤„ç†ï¼Œæ­¤å¤„æ— éœ€é¢å¤–é€»è¾‘

    logger.info("è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")

# --- Helper Functions ---


def run_command(
    command_parts,
    check=False,
    cwd=None,
    capture_output=True,
    stdin=None,
    stderr=None,
    force_shell=False,  # æ–°å¢å‚æ•°ï¼Œæ˜ç¡®æ§åˆ¶shellä½¿ç”¨
    timeout=30  # æ–°å¢è¶…æ—¶å‚æ•°ï¼Œé»˜è®¤30ç§’
):
    """Runs a command and returns its output, error, and return code.

    Args:
        command_parts: Command as list (preferred) or string
        check: Whether to raise exception on non-zero exit
        cwd: Working directory
        capture_output: Whether to capture stdout/stderr
        stdin: Input for the command
        stderr: How to handle stderr
        force_shell: Explicitly force shell=True (use with caution)
        timeout: Command timeout in seconds (default: 30)
    """
    try:
        # ä¼˜å…ˆä½¿ç”¨åˆ—è¡¨å½¢å¼ï¼Œé¿å…shell=Trueçš„é—®é¢˜
        if isinstance(command_parts, str) and not force_shell:
            # å°è¯•æ™ºèƒ½æ‹†åˆ†å‘½ä»¤å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
            import shlex
            try:
                command_parts = shlex.split(command_parts)
                is_shell = False
                logger.info(
                    f"Auto-converted string command to list: {command_parts}")
            except ValueError:
                # å¦‚æœæ‹†åˆ†å¤±è´¥ï¼Œå›é€€åˆ°shell=True
                is_shell = True
                logger.warning(
                    f"Failed to parse command string, using shell=True: {command_parts}")
        elif isinstance(command_parts, str) and force_shell:
            is_shell = True
            logger.info(f"Explicitly using shell=True: {command_parts}")
        else:
            is_shell = False

        # åœ¨Windowsä¸Šï¼Œå¯¹äºnpm/npxå‘½ä»¤éœ€è¦ä½¿ç”¨shell=True
        if sys.platform == "win32" and isinstance(
                command_parts, list) and len(command_parts) > 0:
            if command_parts[0] in ['npm', 'npx', 'node']:
                is_shell = True
                logger.info(
                    f"Using shell=True for Windows npm/npx command: {command_parts}")

        effective_cwd = (
            cwd if cwd is not None else PROJECT_ROOT
        )  # Default to project root if not specified

        # åœ¨Windowsä¸Šï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ç¼–ç 
        env = os.environ.copy()
        if sys.platform == "win32":
            env['PYTHONIOENCODING'] = 'utf-8'

        result = subprocess.run(
            command_parts,
            capture_output=capture_output,
            shell=is_shell,
            check=check,
            cwd=effective_cwd,
            stdin=stdin,
            stderr=stderr,
            env=env,  # ä¼ é€’ç¯å¢ƒå˜é‡
            text=True,  # ç›´æ¥è¿”å›å­—ç¬¦ä¸²è€Œä¸æ˜¯å­—èŠ‚
            encoding='utf-8',  # æ˜ç¡®æŒ‡å®šç¼–ç 
            errors='replace',  # å¤„ç†ç¼–ç é”™è¯¯
            timeout=timeout  # æ·»åŠ è¶…æ—¶æ§åˆ¶
        )

        # ç”±äºä½¿ç”¨äº†text=Trueï¼Œç›´æ¥è·å–å­—ç¬¦ä¸²ç»“æœ
        stdout_res = result.stdout.strip() if result.stdout else ""
        stderr_res = result.stderr.strip() if result.stderr else ""

        return stdout_res, stderr_res, result.returncode
    except FileNotFoundError:
        cmd_str = command_parts[0] if isinstance(
            command_parts, list) else command_parts
        logger.error(f"Command not found: {cmd_str} (in cwd: {effective_cwd})")
        return None, "Command not found", -1
    except subprocess.CalledProcessError as e:
        logger.error(
            f"Command failed with exit code {
                e.returncode}: {command_parts}")
        stdout_res = e.stdout.strip() if e.stdout else ""
        stderr_res = e.stderr.strip() if e.stderr else ""
        return stdout_res, stderr_res, e.returncode
    except subprocess.TimeoutExpired as e:
        logger.error(f"Command timed out after {timeout}s: {command_parts}")
        return None, f"Command timed out after {timeout}s", -3
    except Exception as e:
        logger.error(f"Error running command '{command_parts}': {e}")
        return None, str(e), -2


def invoke_autoformat():
    """Runs code autoformatters."""
    global issues_found
    logger.info("Starting automatic code formatting...")
    # Remove unused imports and variables via autoflake
    logger.info("Running autoflake to remove unused imports and variables...")
    run_command([
        sys.executable,
        "-m",
        "autoflake",
        "--remove-all-unused-imports",
        "--ignore-init-module-imports",
        "--in-place",
        "-r",
        str(PROJECT_ROOT / "src" / "backend"),
        str(PROJECT_ROOT / "scripts"),
    ])
    try:
        importlib.import_module("autopep8")
        cmd = [
            sys.executable,
            "-m",
            "autopep8",
            "--in-place",
            "--recursive",
            "--aggressive",
            "--aggressive",
            str(PROJECT_ROOT),
        ]
        out, err, code = run_command(cmd)
        if code != 0:
            logger.warning(f"autopep8 issues (code {code}): {err}")
    except ModuleNotFoundError:
        logger.warning(
            "autopep8 not installed, skipping PEP8 auto formatting.")
    # 5. JS ESLint fix via npm script
    try:
        frontend_dir = PROJECT_ROOT / "src" / "frontend" / "admin"
        if frontend_dir.exists():
            cmd = ["npm", "run", "lint:fix"]
            out, err, code = run_command(cmd, cwd=str(frontend_dir))
            if code != 0:
                logger.warning(f"npm lint:fix issues (code {code}): {err}")
        else:
            logger.info(
                "Frontend admin directory not found for JS auto-format.")
    except Exception as e:
        logger.warning(f"JS auto-fix (npm run lint:fix) failed: {e}")
    logger.info("Automatic code formatting completed.")
    # Update report section
    update_report(
        "Automatic Code Formatting",
        "âœ”ï¸ Applied autoflake, isort, black (if available), eslint --fix",
    )
    return True


# --- Report Functions ---


def initialize_report():
    """Creates the initial report file with header."""
    global REPORT_FILE
    try:
        project_name = PROJECT_ROOT.name
        report_header = """\
# {project_name} Work Completion Report (Python)

**Generation Time**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Project Name**: {project_name}
**Report ID**: {TIMESTAMP}

## Table of Contents

1. [Dependency Check](#dependency-check)
2. [Directory Structure Check](#directory-structure-check)
# ... (Other sections to be added later)

"""
        with open(REPORT_FILE, "w", encoding="utf-8") as f:
            f.write(report_header)
        logger.info(f"Initializing report: {REPORT_FILE}")
        return True
    except Exception as e:
        logger.error(f"Failed to initialize report: {e}")
        return False


def update_report(section, content):
    """Appends a new section with content to the report file."""
    global REPORT_FILE
    try:
        section_header = f"{NL}{NL}## {section}{NL}{NL}"
        full_content = section_header + content
        with open(REPORT_FILE, "a", encoding="utf-8") as f:
            f.write(full_content)
        return True
    except Exception as e:
        logger.error(f"Failed to update report section '{section}': {e}")
        return False


# --- Dependency Check Functions ---


def test_python():
    """Checks if Python is installed and executable."""
    stdout, stderr, code = run_command([sys.executable, "--version"])
    if code == 0 and stdout and "Python" in stdout:
        logger.info(f"Python installed: {stdout}")
        return True, stdout
    else:
        logger.warning(
            f"Python not found or version check failed. Code: {code}, Stderr: {stderr}")
        return False, "-"


def test_node():
    """Checks if Node.js is installed."""
    stdout, stderr, code = run_command(["node", "--version"])
    if code == 0 and stdout and stdout.startswith("v"):
        logger.info(f"Node.js installed: {stdout}")
        return True, stdout
    else:
        logger.warning(
            f"Node.js not found or version check failed. Code: {code}, Stderr: {stderr}")
        return False, "-"


def test_flake8():
    """Checks if flake8 is installed."""
    # Prefer checking within the project's Python environment context
    # Ensure we use the same python executable that runs this script
    command = [sys.executable, "-m", "flake8", "--version"]
    logger.info(f"Running command to check flake8: {' '.join(command)}")

    # Explicitly set stdin to DEVNULL and cwd for robustness
    try:
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            encoding="utf-8",
            cwd=PROJECT_ROOT,  # Ensure correct working directory
            stdin=subprocess.DEVNULL,  # Explicitly set stdin
            check=False,  # Don't raise error on non-zero exit
        )
        stdout = result.stdout.strip()
        stderr = result.stderr.strip()
        code = result.returncode
    except FileNotFoundError:
        logger.error(f"Command not found: {command[0]}")
        return False, "-"
    except Exception as e:
        logger.error(f"Error running command '{command}': {e}")
        return False, "-"

    # Debugging output
    logger.debug(
        f"flake8 check - Code: {code}, Stdout: '{stdout}', Stderr: '{stderr}'")

    # Improve checking logic: Check return code and if stdout contains version
    # info
    if code == 0 and stdout:
        # Try to extract a cleaner version string
        version_line = stdout.splitlines()[0]
        # Extract version like X.Y.Z or similar patterns
        version_match_obj = re.search(r"^(\d+\.\d+(\.\d+)?)", version_line)
        version_match = (version_match_obj.group(
            1) if version_match_obj else version_line.split()[0])
        logger.info(
            f"flake8 installed: {version_match} (Full output: {version_line})")
        return True, version_match
    else:
        logger.warning(
            f"flake8 not found or version check failed. Code: {code}, Stdout: '{stdout}', Stderr: '{stderr}'")
        return False, "-"


def test_eslint():
    """Checks if ESLint is installed (globally via npx or locally)."""
    # 1. Check via npx first (most reliable for installed packages)
    logger.info("Checking ESLint via npx...")
    stdout, stderr, code = run_command(
        ["npx", "eslint", "--version"],
        timeout=60  # ESLintæ£€æŸ¥ä½¿ç”¨æ›´é•¿è¶…æ—¶æ—¶é—´
    )  # ä½¿ç”¨åˆ—è¡¨å½¢å¼é¿å…shell=True
    if code == 0 and stdout and stdout.strip().startswith("v"):
        logger.info(f"ESLint available via npx: {stdout.strip()}")
        return True, stdout.strip()

    # 2. Check local node_modules as fallback
    local_eslint_path = PROJECT_ROOT / "node_modules" / ".bin" / "eslint"

    # Windowsç³»ç»Ÿä¸‹æ£€æŸ¥.cmdæ–‡ä»¶
    if sys.platform == "win32":
        local_eslint_cmd = local_eslint_path.with_suffix(".cmd")
        if local_eslint_cmd.exists():
            stdout, stderr, code = run_command(
                [str(local_eslint_cmd), "--version"],
                timeout=60)
            if code == 0 and stdout and stdout.strip().startswith("v"):
                logger.info(
                    f"ESLint installed in project (Windows): {
                        stdout.strip()}")
                return True, stdout.strip()
    else:
        # Unixç³»ç»Ÿä¸‹æ£€æŸ¥åŸå§‹æ–‡ä»¶
        if local_eslint_path.exists():
            stdout, stderr, code = run_command(
                [str(local_eslint_path), "--version"],
                timeout=60)
            if code == 0 and stdout and stdout.strip().startswith("v"):
                logger.info(
                    f"ESLint installed in project (Unix): {
                        stdout.strip()}")
                return True, stdout.strip()

    logger.warning(
        f"ESLint not found locally or via npx. Code: {code}, Stderr: {stderr}"
    )
    return False, "-"


def test_dependencies():
    """Runs all dependency checks and updates the report."""
    global warnings_found
    logger.info("Checking dependencies...")
    results = {}
    results["Python"] = test_python()
    results["Node.js"] = test_node()
    results["flake8"] = test_flake8()
    results["ESLint"] = test_eslint()

    report_content = f"### Dependency Check Results{NL}{NL}"
    report_content += f"| Dependency | Status | Version |{NL}"
    report_content += f"|------------|--------|---------|{NL}"

    all_deps_ok = True
    required_deps = ["Python", "Node.js"]  # Define essential dependencies

    for name, (installed, version) in results.items():
        status_icon = "âœ…" if installed else (
            "âŒ" if name in required_deps else "âš ï¸")
        status_text = "Installed" if installed else "Not Installed"
        report_content += f"| {name} | {status_icon} {status_text} | {version} |{NL}"
        if not installed:
            if name in required_deps:
                all_deps_ok = False
            else:
                warnings_found += 1  # Non-required dependency missing is a warning
        else:
            logger.info(f"{name} dependency check passed: {version}")

    update_report("Dependency Check", report_content)

    # Store dependency check results to memory
    if 'mcp_tools' in globals():
        dependency_stats = {
            "timestamp": datetime.now().isoformat(),
            "all_deps_ok": all_deps_ok,
            "results": {
                name: {
                    "installed": installed,
                    "version": version} for name,
                (installed,
                 version) in results.items()},
            "required_deps": required_deps,
            "warnings_count": sum(
                1 for name,
                (installed,
                 version) in results.items() if not installed and name not in required_deps)}
        mcp_tools.store_memory(
            "dependency_check_results",
            dependency_stats,
            "dependencies")

    if not all_deps_ok:
        logger.warning("Some required dependencies are not installed.")
    else:
        logger.info("All required dependencies are installed.")

    return all_deps_ok


# --- Dependency Check Functions End ---

# --- Directory Structure Check Start ---


def get_dir_tree(
    startpath: Path,
    max_depth: int = 3,
    indent_char: str = "    ",
    exclude_dirs=None,
):
    """Generates a directory tree structure string."""
    if exclude_dirs is None:
        exclude_dirs = {".git", "__pycache__", "node_modules", ".venv", "venv"}

    tree = []

    def recurse(current_path: Path, depth: int, prefix: str = ""):
        if depth > max_depth:
            return

        try:
            items = sorted(
                list(current_path.iterdir()),
                key=lambda p: (p.is_file(), p.name.lower()),
            )
        except PermissionError:
            tree.append(f"{prefix}â””â”€â”€ [Permission Denied]")
            return
        except FileNotFoundError:
            tree.append(
                f"{prefix}â””â”€â”€ [Not Found]"
            )  # Should not happen if startpath exists
            return

        pointers = ["â”œâ”€â”€ "] * (len(items) - 1) + ["â””â”€â”€ "]
        for pointer, item in zip(pointers, items):
            if item.name in exclude_dirs:
                continue

            icon = "ğŸ“" if item.is_dir() else "ğŸ“„"
            tree.append(f"{prefix}{pointer}{icon} {item.name}")

            if item.is_dir():
                extension = indent_char if pointer == "â”œâ”€â”€ " else " " * \
                    len(indent_char)
                recurse(item, depth + 1, prefix + extension)

    # Start recursion from the root of the startpath for the tree structure
    tree.append(f"ğŸ“ {startpath.name}/")  # Add the root dir itself
    recurse(startpath, 0)
    return NL.join(tree)


def get_file_list_details(startpath: Path, exclude_dirs=None):
    """Generates a markdown table with file details recursively."""
    if exclude_dirs is None:
        exclude_dirs = {".git", "__pycache__", "node_modules", ".venv", "venv"}

    details = []
    details.append(f"| File Path | Type | Size (KB) | Last Modified |{NL}")
    details.append(f"|-----------|------|-----------|---------------|{NL}")

    for item in startpath.rglob("*"):
        # Check if the item or any of its parents are in the exclude list
        if any(part in exclude_dirs for part in item.relative_to(startpath).parts):
            continue
        # Also check the top-level item itself if it's in the root
        if item.parent == startpath and item.name in exclude_dirs:
            continue

        try:
            rel_path = item.relative_to(startpath).as_posix()
            item_type = "Directory" if item.is_dir() else "File"
            size = "N/A"
            last_modified = "N/A"
            if item.is_file():
                size = f"{item.stat().st_size / 1024:.2f}"
            last_modified = datetime.fromtimestamp(
                item.stat().st_mtime).strftime("%Y-%m-%d %H:%M:%S")
            details.append(
                f"| {rel_path} | {item_type} | {size} | {last_modified} |{NL}"
            )
        except Exception as e:
            logger.warning(f"Could not get details for item {item}: {e}")
            details.append(
                f"| {
                    item.relative_to(startpath).as_posix()} | Error | Error | Error |{NL}")

    return "".join(details)


def check_config_file_uniqueness():
    """æ£€æŸ¥é…ç½®æ–‡ä»¶çš„å”¯ä¸€æ€§ï¼Œç¡®ä¿é…ç½®æ–‡ä»¶åªåœ¨å…è®¸çš„ä½ç½®å­˜åœ¨ã€‚"""
    global issues_found, dir_spec
    logger.info("æ£€æŸ¥é…ç½®æ–‡ä»¶å”¯ä¸€æ€§...")

    # ä»é…ç½®åŠ è½½å”¯ä¸€é…ç½®æ–‡ä»¶è§„åˆ™
    unique_config_files = dir_spec.get("unique_config_files", [])
    if not unique_config_files:
        logger.warning("æœªæ‰¾åˆ°å”¯ä¸€é…ç½®æ–‡ä»¶è§„åˆ™ï¼Œè·³è¿‡é…ç½®æ–‡ä»¶å”¯ä¸€æ€§æ£€æŸ¥ã€‚")
        return True

    issues_detected = False
    report_lines = []

    for config_file in unique_config_files:
        file_name = config_file.get("name")
        allowed_paths = config_file.get(
            "allowed_paths", config_file.get(
                "allowed_path", ["."]))
        description = config_file.get("description", "")

        # ç¡®ä¿allowed_pathsæ˜¯åˆ—è¡¨
        if isinstance(allowed_paths, str):
            allowed_paths = [allowed_paths]

        if not file_name or not allowed_paths:
            continue

        # æ„å»ºæ‰€æœ‰å…è®¸çš„å®Œæ•´è·¯å¾„
        allowed_full_paths = []
        for allowed_path in allowed_paths:
            if allowed_path == ".":
                allowed_full_paths.append(PROJECT_ROOT / file_name)
            else:
                allowed_full_paths.append(
                    PROJECT_ROOT / allowed_path / file_name)

        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        found_files = list(PROJECT_ROOT.glob(f"**/{file_name}"))

        # æ’é™¤å…è®¸çš„è·¯å¾„
        disallowed_files = [
            f for f in found_files if f not in allowed_full_paths]

        if disallowed_files:
            issues_detected = True
            issues_found += len(disallowed_files)
            disallowed_paths = [str(f.relative_to(PROJECT_ROOT))
                                for f in disallowed_files]
            error_msg = f"å‘ç°é‡å¤çš„é…ç½®æ–‡ä»¶ {file_name}({description})ï¼Œåº”è¯¥åªå­˜åœ¨äº {', '.join(allowed_paths)}ï¼Œä½†åœ¨ä»¥ä¸‹ä½ç½®æ‰¾åˆ°: {', '.join(disallowed_paths)}"
            logger.error(error_msg)
            report_lines.append(error_msg)

        # æ£€æŸ¥å…è®¸è·¯å¾„ä¸­æ˜¯å¦æœ‰å¤šä¸ªæ–‡ä»¶
        valid_files = [f for f in found_files if f in allowed_full_paths]
        if len(valid_files) > 1:
            issues_detected = True
            issues_found += len(valid_files) - 1
            valid_paths = [str(f.relative_to(PROJECT_ROOT))
                           for f in valid_files]
            error_msg = f"åœ¨å…è®¸çš„ä½ç½®å‘ç°å¤šä¸ª {file_name} æ–‡ä»¶: {
                ', '.join(valid_paths)}ã€‚{description}åº”è¯¥åªæœ‰ä¸€ä¸ªã€‚"
            logger.error(error_msg)
            report_lines.append(error_msg)

    if report_lines:
        section = "é…ç½®æ–‡ä»¶å”¯ä¸€æ€§æ£€æŸ¥"
        content = "\n".join(report_lines)
        update_report(section, content)

    return not issues_detected


def check_duplicate_files():
    """æ£€æŸ¥é¡¹ç›®ä¸­çš„é‡å¤æ–‡ä»¶ã€‚"""
    global issues_found, dir_spec
    logger.info("æ£€æŸ¥é‡å¤æ–‡ä»¶...")

    # ä»é…ç½®åŠ è½½é‡å¤æ–‡ä»¶æ£€æµ‹è§„åˆ™
    duplicate_detection = dir_spec.get("duplicate_detection", {})
    patterns = duplicate_detection.get("patterns", [])
    exclude_dirs = duplicate_detection.get("exclude_dirs", [])
    allowed_duplicates = duplicate_detection.get("allowed_duplicates", [])

    if not patterns:
        logger.warning("æœªæ‰¾åˆ°é‡å¤æ–‡ä»¶æ£€æµ‹æ¨¡å¼ï¼Œè·³è¿‡é‡å¤æ–‡ä»¶æ£€æŸ¥ã€‚")
        return True

    issues_detected = False
    report_lines = []

    # æ„å»ºæ’é™¤ç›®å½•çš„å®Œæ•´è·¯å¾„
    exclude_paths = [str(PROJECT_ROOT / d) for d in exclude_dirs]

    # å¯¹æ¯ä¸ªæ¨¡å¼è¿›è¡Œæ£€æŸ¥
    for pattern in patterns:
        # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        matched_files = []
        for path in PROJECT_ROOT.glob(f"**/{pattern}"):
            # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤ç›®å½•ä¸­
            path_excluded = False
            for ex in exclude_paths:
                if str(path).startswith(ex):
                    path_excluded = True
                    break
            if not path_excluded:
                matched_files.append(path)

        # æŒ‰æ–‡ä»¶ååˆ†ç»„
        file_groups = {}
        for file_path in matched_files:
            file_name = file_path.name
            if file_name not in file_groups:
                file_groups[file_name] = []
            file_groups[file_name].append(file_path)

        # æ£€æŸ¥æ¯ä¸ªç»„æ˜¯å¦æœ‰å¤šä¸ªæ–‡ä»¶
        for file_name, paths in file_groups.items():
            if len(paths) > 1 and file_name not in allowed_duplicates:
                issues_detected = True
                issues_found += len(paths) - 1  # å‡1æ˜¯å› ä¸ºåº”è¯¥åªæœ‰ä¸€ä¸ªæ–‡ä»¶
                relative_paths = [str(p.relative_to(PROJECT_ROOT))
                                  for p in paths]
                error_msg = f"å‘ç°é‡å¤æ–‡ä»¶ {file_name}ï¼Œåœ¨ä»¥ä¸‹ä½ç½®: {
                    ', '.join(relative_paths)}"
                logger.error(error_msg)
                report_lines.append(error_msg)

    if report_lines:
        section = "é‡å¤æ–‡ä»¶æ£€æŸ¥"
        content = "\n".join(report_lines)
        update_report(section, content)

    return not issues_detected


def invoke_directory_check():
    """Performs directory structure check and generates structure files using Python."""
    global issues_found, dir_spec
    logger.info(
        "Performing directory structure check (Python implementation)...")

    # æ£€æŸ¥é¡¹ç›®å†…æ˜¯å¦å­˜åœ¨æ—§çš„å¤‡ä»½ç›®å½•ï¼ˆåº”è¯¥ç§»è‡³é¡¹ç›®å¤–ï¼‰
    legacy = PROJECT_ROOT / "AI9-Backups"
    if legacy.exists():
        logger.warning(f"å‘ç°é¡¹ç›®å†…å­˜åœ¨æ—§çš„å¤‡ä»½ç›®å½•ï¼Œåº”ç§»è‡³{STANDARD_BACKUP_DIR}")
        # ä¸è‡ªåŠ¨åˆ é™¤ï¼Œé¿å…æ•°æ®ä¸¢å¤±é£é™©
        logger.info(f"è¯·æ‰‹åŠ¨å°†å†…å®¹è¿ç§»åˆ°æ­£ç¡®çš„å¤‡ä»½ç›®å½•: {STANDARD_BACKUP_DIR}")

    # è°ƒç”¨ä¸¥æ ¼çš„AgentéªŒè¯å‡½æ•°
    structure_ok = agent_verify_structure(
        auto_cleanup=getattr(
            args, 'auto_cleanup', False))
    if not structure_ok:
        logger.error("Agentç›®å½•ç»“æ„éªŒè¯å¤±è´¥")
        return False

    # æ£€æŸ¥é…ç½®æ–‡ä»¶å”¯ä¸€æ€§
    config_uniqueness_ok = check_config_file_uniqueness()
    if not config_uniqueness_ok:
        logger.error("é…ç½®æ–‡ä»¶å”¯ä¸€æ€§æ£€æŸ¥å¤±è´¥ï¼Œå‘ç°é‡å¤çš„é…ç½®æ–‡ä»¶ã€‚")

    # æ£€æŸ¥é‡å¤æ–‡ä»¶
    no_duplicates_ok = check_duplicate_files()
    if not no_duplicates_ok:
        logger.error("é‡å¤æ–‡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œå‘ç°é‡å¤æ–‡ä»¶ã€‚")

    # Store directory check results to memory
    if 'mcp_tools' in globals():
        directory_check_stats = {
            "timestamp": datetime.now().isoformat(),
            "structure_ok": structure_ok,
            "config_uniqueness_ok": config_uniqueness_ok,
            "no_duplicates_ok": no_duplicates_ok,
            "overall_success": structure_ok and config_uniqueness_ok and no_duplicates_ok,
            "legacy_backup_exists": legacy.exists() if 'legacy' in locals() else False}
        mcp_tools.store_memory(
            "directory_check_results",
            directory_check_stats,
            "directory_structure")

    # åªæœ‰å½“æ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡æ—¶æ‰è¿”å›True
    return structure_ok and config_uniqueness_ok and no_duplicates_ok


# --- Directory Structure Check End ---

# --- Working Directory Cleanliness Check Start ---


def invoke_workdir_clean_check():
    """Checks for scattered files/dirs in the working directory (parent of project root)."""
    global warnings_found
    logger.info("Checking working directory cleanliness...")
    working_directory = PROJECT_ROOT.parent

    # Define allowed top-level items in the working directory (names or
    # patterns)
    allowed_items = {
        ".cursor",  # Directory
        ".git",  # Directory
        "docs",  # Directory
        "logs",  # Directory
        ".trae",  # Directory
        # Allowed files
        ".env",  # Environment file
        ".flake8",  # Flake8 configuration file
        "README.md",
        "LICENSE",
        # Allowed patterns
        ".git*",  # Files like .gitignore, .gitattributes
        "*.lnk",  # Shortcut files
    }

    scattered_items_details = []
    try:
        for item in working_directory.iterdir(
        ):  # Iterate through items in parent dir
            is_allowed = False
            # Direct name match
            if item.name in allowed_items:
                is_allowed = True
            else:
                # Pattern match for files
                if item.is_file():
                    for pattern in allowed_items:
                        if (
                            "*" in pattern or "?" in pattern
                        ):  # Simple check for pattern characters
                            # Use fnmatch for pattern matching if needed, pathlib's match is anchorbased
                            # For simple cases like *.lnk or .git*, name
                            # matching works
                            if item.match(pattern):
                                is_allowed = True
                                break
                # No need for complex dir pattern matching here usually

            if not is_allowed:
                item_type = "Directory" if item.is_dir() else "File"
                size = "N/A"
                last_modified = "N/A"
                try:
                    stat = item.stat()
                    if item.is_file():
                        size = f"{stat.st_size / 1024:.2f}"
                    last_modified = datetime.fromtimestamp(
                        stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
                except Exception as stat_e:
                    logger.warning(
                        f"Could not get stats for scattered item {
                            item.name}: {stat_e}")

                scattered_items_details.append(
                    {
                        "name": item.name,
                        "type": item_type,
                        "size": size,
                        "modified": last_modified,
                    }
                )
                logger.warning(
                    f"Found scattered item in working directory: {item.name}"
                )
                warnings_found += 1

        # Update report
        report_content = ""
        if scattered_items_details:
            report_content += f"âš ï¸ Found {
                len(scattered_items_details)} scattered items in the working directory ({working_directory}):{NL}{NL}"
            report_content += f"| Name | Type | Size (KB) | Last Modified |{NL}"
            report_content += f"|------|------|-----------|---------------|{NL}"
            for item in scattered_items_details:
                report_content += f"| {
                    item['name']} | {
                    item['type']} | {
                    item['size']} | {
                    item['modified']} |{NL}"
            report_content += f"{NL}### Suggestions{NL}{NL}"
            report_content += f"- Script files should be moved to appropriate subdirectories under 'scripts/'{NL}"
            report_content += f"- Configuration files should be moved to appropriate subdirectories under 'config/'{NL}"
            report_content += (
                f"- Log files should be moved to the 'logs/' directory{NL}"
            )
            report_content += f"- Temporary files should be moved to the 'temp/' directory or deleted{NL}"
        else:
            logger.info(
                "Working directory is clean, no scattered files found.")
            report_content = "âœ… Working directory is clean, no scattered files found."

        update_report("Working Directory Cleanliness Check", report_content)
        # Return True if clean (no scattered items)
        return len(scattered_items_details) == 0

    except Exception as e:
        logger.error(f"Error checking working directory cleanliness: {e}")
        update_report(
            "Working Directory Cleanliness Check",
            f"âŒ Error occurred while checking working directory cleanliness: {e}",
        )
        return False  # Treat error as failure


def load_whitelist_from_structure_file():
    """ä»é…ç½®æ–‡ä»¶ä¸­åŠ è½½ç™½åå•
    """
    # ç›´æ¥ä»é…ç½®æ–‡ä»¶åŠ è½½ç™½åå•
    allowed = set(dir_spec.get("required_dirs", []))
    # æ·»åŠ required_filesåˆ°ç™½åå•
    allowed.update(dir_spec.get("required_files", []))
    disallowed = set(dir_spec.get("disallowed_top_dirs", []))

    logger.info(f"ä»é…ç½®æ–‡ä»¶åŠ è½½ç™½åå•: {sorted(allowed)}")
    return allowed, disallowed


# --- Agent Verification Function Start ---
def agent_verify_structure(auto_cleanup=False):
    """Agent è‡ªåŠ¨æ£€æŸ¥é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„é¡¶çº§ç›®å½•å’Œæ–‡ä»¶æ˜¯å¦ç¬¦åˆè§„èŒƒ
    ä¸¥æ ¼ç™½åå•æœºåˆ¶ï¼šåªå…è®¸ç™½åå•å†…å®¹å­˜åœ¨ï¼Œå‘ç°è¿è§„å†…å®¹åå®Œæ•´æ£€æŸ¥å¹¶å¯é€‰æ‹©è‡ªåŠ¨æ¸…ç†
    ç°åœ¨ä»é¡¹ç›®ç›®å½•ç»“æ„é™„è¡¨æ–‡ä»¶è¯»å–ç™½åå•

    Args:
        auto_cleanup (bool): æ˜¯å¦è‡ªåŠ¨æ¸…ç†è¿è§„ç›®å½•åˆ°é¡¹ç›®å¤–éƒ¨
    """
    global issues_found, warnings_found, dir_spec
    logger.info("AgentéªŒè¯: å¼€å§‹ä¸¥æ ¼æ£€æŸ¥é¡¹ç›®æ ¹ç›®å½•ç»“æ„...")

    # ä»é¡¹ç›®ç›®å½•ç»“æ„é™„è¡¨æ–‡ä»¶è·å–ç™½åå•å’Œé»‘åå•
    allowed, disallowed = load_whitelist_from_structure_file()

    # æ£€æŸ¥æ‰€æœ‰å®é™…å­˜åœ¨çš„é¡¹ç›®
    actual_items = []
    for item in PROJECT_ROOT.iterdir():
        actual_items.append(item.name)

    # æ”¶é›†æ‰€æœ‰è¿è§„é¡¹ç›®ï¼Œè€Œä¸æ˜¯ç«‹å³é€€å‡º
    all_violations = []

    # æ£€æŸ¥æ˜¯å¦æœ‰é»‘åå•é¡¹ç›®å­˜åœ¨
    found_disallowed = [name for name in actual_items if name in disallowed]
    if found_disallowed:
        all_violations.extend([(name, "é»‘åå•é¡¹ç›®") for name in found_disallowed])
        logger.error(f"âŒ å‘ç°ç¦æ­¢å­˜åœ¨çš„ç›®å½•/æ–‡ä»¶: {found_disallowed}")

    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸åœ¨ç™½åå•çš„é¡¹ç›®
    non_hidden_items = [
        name for name in actual_items if not name.startswith(".")]
    invalid = [name for name in non_hidden_items if name not in allowed]
    if invalid:
        all_violations.extend([(name, "ä¸åœ¨ç™½åå•") for name in invalid])
        logger.error(f"âŒ å‘ç°ä¸åœ¨ç™½åå•ä¸­çš„é¡¶çº§é¡¹: {invalid}")

    # å¦‚æœæœ‰è¿è§„é¡¹ç›®ï¼Œå¤„ç†å®ƒä»¬
    if all_violations:
        issues_found += len(all_violations)

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        error_msg = f"âŒ ä¸¥é‡é”™è¯¯ï¼šå‘ç° {len(all_violations)} ä¸ªè¿è§„ç›®å½•/æ–‡ä»¶"
        logger.error(error_msg)
        logger.error(f"è¿™äº›å†…å®¹åº”è¯¥ç§»åŠ¨åˆ°é¡¹ç›®å¤–éƒ¨ï¼ˆ{STANDARD_CLEANUP_DIR.parent}ï¼‰")

        # æ›´æ–°æŠ¥å‘Š
        section = "Agent Directory Verification"
        content = f"{error_msg}\n\nâš ï¸ å‘ç°ä»¥ä¸‹è¿è§„å†…å®¹ï¼š\n"
        for item, reason in all_violations:
            content += f"- {item} ({reason})\n"

        # å¦‚æœå¯ç”¨è‡ªåŠ¨æ¸…ç†
        if auto_cleanup:
            import shutil
            cleanup_target = STANDARD_CLEANUP_DIR.parent
            cleanup_success = []
            cleanup_failed = []

            for item, reason in all_violations:
                item_path = PROJECT_ROOT / item
                target_path = cleanup_target / item

                # å¦‚æœç›®æ ‡è·¯å¾„å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³åç¼€
                if target_path.exists():
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    if target_path.is_dir():
                        target_path = cleanup_target / f"{item}_{timestamp}"
                    else:
                        stem = target_path.stem
                        suffix = target_path.suffix
                        target_path = cleanup_target / \
                            f"{stem}_{timestamp}{suffix}"

                try:
                    if item_path.is_dir():
                        shutil.move(str(item_path), str(target_path))
                    else:
                        shutil.move(str(item_path), str(target_path))
                    cleanup_success.append(item)
                    logger.info(f"âœ… å·²è‡ªåŠ¨ç§»åŠ¨è¿è§„é¡¹ç›®: {item} -> {target_path}")
                except Exception as e:
                    cleanup_failed.append((item, str(e)))
                    logger.error(f"âŒ è‡ªåŠ¨æ¸…ç†å¤±è´¥: {item} - {e}")

            content += "\nğŸ”§ è‡ªåŠ¨æ¸…ç†ç»“æœï¼š\n"
            content += f"- æˆåŠŸæ¸…ç†: {len(cleanup_success)} é¡¹\n"
            content += f"- æ¸…ç†å¤±è´¥: {len(cleanup_failed)} é¡¹\n"

            if cleanup_failed:
                content += "\nâŒ æ¸…ç†å¤±è´¥çš„é¡¹ç›®ï¼š\n"
                for item, error in cleanup_failed:
                    content += f"- {item}: {error}\n"
                content += "\nè¯·æ‰‹åŠ¨æ¸…ç†å¤±è´¥çš„é¡¹ç›®åé‡æ–°è¿è¡Œæ£€æŸ¥ã€‚"
                update_report(section, content)
                import sys
                sys.exit(1)
            else:
                content += "\nâœ… æ‰€æœ‰è¿è§„é¡¹ç›®å·²æˆåŠŸè‡ªåŠ¨æ¸…ç†ã€‚"
                logger.info("âœ… æ‰€æœ‰è¿è§„é¡¹ç›®å·²æˆåŠŸè‡ªåŠ¨æ¸…ç†")
        else:
            content += "\nè¯·æ‰‹åŠ¨æ¸…ç†åé‡æ–°è¿è¡Œæ£€æŸ¥ï¼Œæˆ–ä½¿ç”¨ --auto-cleanup å‚æ•°å¯ç”¨è‡ªåŠ¨æ¸…ç†ã€‚"
            update_report(section, content)
            logger.error("ç³»ç»Ÿæ£€æŸ¥åœæ­¢ï¼Œè¯·å…ˆæ¸…ç†è¿è§„å†…å®¹ï¼")
            import sys
            sys.exit(1)

    # å¦‚æœæ²¡æœ‰è¿è§„é¡¹ç›®
    logger.info("âœ… AgentéªŒè¯: æ‰€æœ‰é¡¶çº§ç›®å½•å’Œæ–‡ä»¶å‡ç¬¦åˆä¸¥æ ¼ç™½åå•è§„èŒƒ")

    # æ›´æ–°æŠ¥å‘Š
    section = "Agent Directory Verification"
    content = "âœ… æ‰€æœ‰é¡¶çº§ç›®å½•å’Œæ–‡ä»¶å‡ç¬¦åˆä¸¥æ ¼ç™½åå•è§„èŒƒ"
    update_report(section, content)
    return True


# --- Agent Verification Function End ---

# --- Document Standardization and Backup Start ---


def write_backup_log(message):
    """Appends a message to the backup log file."""
    try:
        # Ensure log directory exists
        BACKUP_LOG_DIR.mkdir(parents=True, exist_ok=True)

        # Check if log file exists and add header if not
        if not BACKUP_LOG_FILE.exists():
            header = """# AI9é¡¹ç›®å¤‡ä»½æ—¥å¿—

å¤‡ä»½ç±»å‹è¯´æ˜:
- **daily**: æ¯æ—¥è‡ªåŠ¨å¤‡ä»½
- **manual**: æ‰‹åŠ¨è§¦å‘çš„å¤‡ä»½
- **weekly**: æ¯å‘¨ç¨³å®šç‰ˆæœ¬çš„å¤‡ä»½
- **milestone**: é¡¹ç›®é‡Œç¨‹ç¢‘çš„é‡è¦å¤‡ä»½

"""
            BACKUP_LOG_FILE.write_text(header, encoding="utf-8")

        # Append the log entry
        with open(BACKUP_LOG_FILE, "a", encoding="utf-8") as f:
            f.write(message + NL)

    except Exception as e:
        logger.error(f"Failed to write backup log to {BACKUP_LOG_FILE}: {e}")


def cleanup_old_backups():
    """Removes old backup files, keeping the latest N backups."""
    try:
        if not BACKUP_DIR.exists():
            return  # No backup directory, nothing to clean

        backup_files = sorted(
            BACKUP_DIR.glob("Project-Backup-*.zip"),
            key=lambda p: p.stat().st_mtime,  # Sort by modification time
            reverse=True,  # Newest first
        )

        if len(backup_files) > BACKUP_RETENTION_COUNT:
            files_to_delete = backup_files[BACKUP_RETENTION_COUNT:]
            logger.info(
                f"Found {
                    len(backup_files)} backups. Keeping {BACKUP_RETENTION_COUNT}, deleting {
                    len(files_to_delete)}.")
            for file_path in files_to_delete:
                try:
                    file_path.unlink()
                    logger.info(f"Deleted old backup: {file_path.name}")
                except Exception as e:
                    logger.error(
                        f"Failed to delete old backup {
                            file_path.name}: {e}")
        else:
            logger.info(
                f"Found {
                    len(backup_files)} backups. No old backups to delete (retention count: {BACKUP_RETENTION_COUNT})."
            )

    except Exception as e:
        logger.error(f"Error during old backup cleanup: {e}")


def invoke_backup(skip_backup=False):
    """Performs project backup using Python's zipfile module."""
    global issues_found
    logger.info("Performing project backup (Python implementation)...")

    if skip_backup:
        logger.info("Skipping project backup based on parameter.")
        update_report(
            "Project Backup",
            "âš ï¸ Skipping project backup based on parameter.")
        return True

    # å®šä¹‰æ—¶é—´æˆ³ï¼Œç”¨äºGitæäº¤å’ŒZIPå¤‡ä»½
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")

    # ç»¼åˆå¤‡ä»½ç¬¬ä¸€æ­¥ï¼šGit æäº¤ä¸æ¨é€
    try:
        logger.info("Starting Git commit and push backup...")

        # ç¡®ä¿Gitä»“åº“ä¸­å­˜åœ¨ç¬¦åˆGitHubä»“åº“ç»“æ„è§„èŒƒçš„æ–‡ä»¶å¤¹
        git_repo_path = str(BACKUP_DIR / "github_repo")  # ä½¿ç”¨æ­£ç¡®çš„Gitä»“åº“è·¯å¾„
        
        # æ¸…ç†å¹¶é‡æ–°åˆ›å»ºGitä»“åº“ç›®å½•ï¼ˆé™¤äº†.gitç›®å½•ï¼‰
        if os.path.exists(git_repo_path):
            for item in os.listdir(git_repo_path):
                if item != '.git':  # ä¿ç•™.gitç›®å½•
                    item_path = os.path.join(git_repo_path, item)
                    if os.path.isdir(item_path):
                        shutil.rmtree(item_path)
                    else:
                        os.remove(item_path)
        else:
            os.makedirs(git_repo_path, exist_ok=True)
        
        # åŒæ­¥é¡¹ç›®æ–‡ä»¶åˆ°Gitä»“åº“ï¼ˆæ’é™¤bakå’Œlogsç›®å½•ï¼‰
        github_dirs = ["docs", "project", "tools"]
        for dir_name in github_dirs:
            src_dir = os.path.join(PROJECT_ROOT, dir_name)
            dst_dir = os.path.join(git_repo_path, dir_name)
            if os.path.exists(src_dir):
                shutil.copytree(src_dir, dst_dir, ignore=shutil.ignore_patterns('*.log', '*.tmp', '__pycache__'))
                logger.info(f"å·²åŒæ­¥ç›®å½•åˆ°Gitä»“åº“ï¼š{dir_name}")
        
        # åŒæ­¥æ ¹ç›®å½•çš„é‡è¦æ–‡ä»¶
        root_files = ["README.md", "requirements.txt", ".gitignore", "pyproject.toml"]
        for file_name in root_files:
            src_file = os.path.join(PROJECT_ROOT, file_name)
            dst_file = os.path.join(git_repo_path, file_name)
            if os.path.exists(src_file):
                shutil.copy2(src_file, dst_file)
                logger.info(f"å·²åŒæ­¥æ–‡ä»¶åˆ°Gitä»“åº“ï¼š{file_name}")

        # åœ¨Gitä»“åº“ç›®å½•ä¸­æ‰§è¡ŒGitå‘½ä»¤
        stdout, stderr, code = run_command(
            ["git", "add", "--all"], cwd=git_repo_path)
        if code != 0:
            logger.warning(f"Git add returned code {code}: {stderr}")
        stdout, stderr, code = run_command(
            ["git", "commit", "-m", f"è‡ªåŠ¨å¤‡ä»½: {timestamp}"], cwd=git_repo_path
        )
        if code == 0:
            logger.info("Git commit succeeded")
        elif "nothing to commit" in stderr.lower():
            logger.info("No changes to commit")
        else:
            logger.warning(f"Git commit returned code {code}: {stderr}")
        # è·å–å½“å‰åˆ†æ”¯
        stdout, stderr, code = run_command(
            ["git", "rev-parse", "--abbrev-re", "HEAD"], cwd=git_repo_path)
        current_branch = stdout.strip() if code == 0 else "main"
        stdout, stderr, code = run_command(
            ["git", "push", "origin", current_branch], cwd=git_repo_path)
        if code == 0:
            logger.info("Git push succeeded")
            update_report("GitHub Backup", "âœ… GitHub push succeeded")
        else:
            logger.error(f"Git push failed with code {code}: {stderr}")
            update_report(
                "GitHub Backup",
                f"âŒ Git push failed with code {code}: {stderr}",
            )
            issues_found += 1
    except Exception as e:
        logger.error(f"Error during Git backup: {e}")
        update_report("GitHub Backup", f"âŒ GitHub backup execution error: {e}")
        issues_found += 1

        # Store Git backup failure to memory
        if 'mcp_tools' in globals():
            mcp_tools.store_memory("git_backup_failure", {
                "timestamp": datetime.now().isoformat(),
                "error_message": str(e),
                "git_repo_path": str(git_repo_path),
                "current_branch": current_branch if 'current_branch' in locals() else "unknown"
            }, "git_errors")

    # ZIP å¤‡ä»½å‚æ•°å®šä¹‰
    backup_filename = f"Project-Backup-{timestamp}.zip"
    backup_path = BACKUP_DIR / backup_filename

    # Ensure backup directory exists
    try:
        BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logger.error(f"Failed to create backup directory {BACKUP_DIR}: {e}")
        update_report(
            "Project Backup",
            f"âŒ Failed to create backup directory: {e}")
        issues_found += 1
        return False

    # Define exclusion patterns (optimized for performance)
    # Directory-level exclusions for early pruning
    exclude_dirs = {
        "node_modules", ".vscode", ".idea", ".git", "venv", "env", 
        "__pycache__", "dist", "build", "backups", ".pytest_cache",
        ".tox", ".mypy_cache"
    }
    
    # File-level exclusions
    exclude_file_patterns = [
        "*.log", "*.pyc", "*.pyo", "*.tmp", "*.temp", "*.cache",
        ".coverage", "coverage.xml", "*.zip", "*.tar.gz", "*.rar", "*.7z"
    ]
    
    # Special directories to preserve structure only
    structure_only_dirs = {"bak", "logs"}
    
    # Standard subdirectories to preserve in structure-only dirs
    standard_bak_dirs = {'config', 'github_repo', 'ä¸“é¡¹å¤‡ä»½', 'è¿ç§»å¤‡ä»½'}
    # ä»é¡¹ç›®è§„èŒƒè·å–logsæ ‡å‡†å­ç›®å½•ï¼ˆæ ¹æ®è§„èŒƒä¸æµç¨‹.mdä¸­çš„å®šä¹‰ï¼‰
    standard_logs_dirs = {'archive', 'å…¶ä»–æ—¥å¿—', 'å·¥ä½œè®°å½•', 'æ£€æŸ¥æŠ¥å‘Š'}

    logger.info(f"Creating backup: {backup_path}")
    processed_files = 0
    added_files = 0
    
    def should_exclude_file(filename):
        """Check if file should be excluded based on patterns"""
        for pattern in exclude_file_patterns:
            if fnmatch.fnmatch(filename, pattern):
                return True
        return False
    
    def should_preserve_structure_dir(rel_path, dirname):
        """Check if directory structure should be preserved"""
        if rel_path == "bak" or rel_path == "logs":
            return True
        if rel_path.startswith("bak/") and dirname in standard_bak_dirs:
            return True
        if rel_path.startswith("logs/") and dirname in standard_logs_dirs:
            return True
        return False
    
    try:
        logger.debug("Opening zip file for writing...")
        with zipfile.ZipFile(backup_path, "w", zipfile.ZIP_DEFLATED) as zipf:
            logger.debug("Starting optimized directory walk...")
            
            # åªå¤‡ä»½æŒ‡å®šçš„ç›®å½•ï¼šdocs, project, tools
            backup_dirs = ["docs", "project", "tools"]
            
            for backup_dir in backup_dirs:
                backup_dir_path = os.path.join(PROJECT_ROOT, backup_dir)
                if not os.path.exists(backup_dir_path):
                    logger.warning(f"å¤‡ä»½ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {backup_dir}")
                    continue
                    
                logger.info(f"æ­£åœ¨å¤‡ä»½ç›®å½•: {backup_dir}")
                
                for root, dirs, files in os.walk(backup_dir_path):
                    # Calculate relative path
                    rel_root = os.path.relpath(root, PROJECT_ROOT)
                    if rel_root == ".":
                        rel_root = ""
                    
                    # Filter out excluded directories (modify dirs in-place for pruning)
                    dirs[:] = [d for d in dirs if d not in exclude_dirs]
                    
                    # Process files in current directory
                    for filename in files:
                        processed_files += 1
                        if processed_files % 1000 == 0:  # Less frequent but more meaningful updates
                            logger.info(f"æ­£åœ¨æ‰«ææ–‡ä»¶... å·²å¤„ç† {processed_files} ä¸ªé¡¹ç›®ï¼Œå·²æ·»åŠ  {added_files} ä¸ªæ–‡ä»¶åˆ°å¤‡ä»½")
                        
                        # Skip excluded files
                        if should_exclude_file(filename):
                            continue
                        
                        # Build file path
                        file_path = os.path.join(root, filename)
                        rel_file_path = os.path.join(rel_root, filename) if rel_root else filename
                        
                        # Skip files in structure-only directories
                        if any(rel_file_path.startswith(d + os.sep) or rel_file_path.startswith(d + "/") 
                               for d in structure_only_dirs):
                            continue
                        
                        try:
                            file_size = os.path.getsize(file_path)
                            zipf.write(file_path, rel_file_path.replace(os.sep, "/"))
                            added_files += 1
                            if added_files % 100 == 0:
                                logger.info(f"å·²æ·»åŠ  {added_files} ä¸ªæ–‡ä»¶åˆ°å¤‡ä»½")
                            logger.debug(f"Added to backup: {rel_file_path} ({file_size} bytes)")
                        except Exception as e:
                            logger.warning(f"Failed to add {file_path} to backup: {e}")
                    
                    # Add structure-only directories
                    if rel_root and should_preserve_structure_dir(rel_root, os.path.basename(root)):
                        try:
                            zipf.writestr(rel_root.replace(os.sep, "/") + "/", "")
                            logger.debug(f"Added directory structure: {rel_root}/")
                        except Exception as e:
                            logger.warning(f"Failed to add directory structure {rel_root}: {e}")
            
            # å¤‡ä»½æ ¹ç›®å½•çš„é‡è¦æ–‡ä»¶
            logger.info("æ­£åœ¨å¤‡ä»½æ ¹ç›®å½•é‡è¦æ–‡ä»¶")
            root_files = ["README.md", "requirements.txt", ".gitignore", "pyproject.toml"]
            for file_name in root_files:
                src_file = os.path.join(PROJECT_ROOT, file_name)
                if os.path.exists(src_file):
                    try:
                        file_size = os.path.getsize(src_file)
                        zipf.write(src_file, file_name)
                        added_files += 1
                        logger.info(f"å·²æ·»åŠ æ ¹ç›®å½•æ–‡ä»¶åˆ°å¤‡ä»½: {file_name}")
                        logger.debug(f"Added root file to backup: {file_name} ({file_size} bytes)")
                    except Exception as e:
                         logger.warning(f"Failed to add root file {file_name} to backup: {e}")

        logger.info("å¤‡ä»½è¿›åº¦æ˜¾ç¤ºç»“æŸ")
        logger.info(
            f"å¤‡ä»½å®Œæˆï¼å·²å¤„ç† {processed_files} ä¸ªé¡¹ç›®ï¼ŒæˆåŠŸæ·»åŠ  {added_files} ä¸ªæ–‡ä»¶åˆ°å¤‡ä»½")
        logger.debug(
            f"Finished writing to zip file. Processed: {processed_files}, Added: {added_files}")
        # Backup completed successfully
        backup_size_mb = backup_path.stat().st_size / (1024 * 1024)
        logger.info(
            f"Backup created successfully: {
                backup_path.name} ({
                backup_size_mb:.2f} MB)")
        update_report(
            "Project Backup",
            f"âœ… Project backup created successfully: {
                backup_path.name} ({
                backup_size_mb:.2f} MB)",
        )

        # Write log entry
        log_entry = f"- {timestamp} [daily] å¤‡ä»½å¤§å°: {backup_size_mb:.2f}MB - {backup_path.name}"
        write_backup_log(log_entry)

        # Cleanup old backups
        logger.debug("Starting cleanup of old backups...")
        cleanup_old_backups()
        logger.debug("Finished cleanup of old backups.")

        return True

    except Exception as e:
        logger.error(
            f"Project backup failed during processing: {e}", exc_info=True
        )  # Log traceback
        update_report("Project Backup", f"âŒ Project backup failed: {e}")
        issues_found += 1

        # Store backup failure details to memory
        if 'mcp_tools' in globals():
            mcp_tools.store_memory("backup_failure_details", {
                "timestamp": datetime.now().isoformat(),
                "error_message": str(e),
                "backup_path": str(backup_path),
                "processed_files": processed_files,
                "added_files": added_files,
                "failure_stage": "processing"
            }, "backup_errors")

        # Attempt to delete potentially incomplete backup file
        try:
            if backup_path.exists():
                logger.info(
                    f"Attempting to delete incomplete backup file: {
                        backup_path.name}")
                backup_path.unlink()
        except Exception as del_e:
            logger.error(
                f"Failed to delete incomplete backup file {
                    backup_path.name}: {del_e}")
            # Store cleanup failure to memory
            if 'mcp_tools' in globals():
                mcp_tools.store_memory("backup_cleanup_failure", {
                    "timestamp": datetime.now().isoformat(),
                    "cleanup_error": str(del_e),
                    "backup_path": str(backup_path)
                }, "backup_errors")
        return False


# --- New Project Cleanup Function Start ---


def invoke_project_cleanup_py():
    """Performs recurring project cleanup tasks (Python implementation)."""
    global issues_found, warnings_found
    logger.info("Performing project cleanup (Python implementation)...")
    cleanup_success = True  # Initialize success flag for cleanup
    report_content = f"### Project Cleanup Results{NL}{NL}"
    actions_taken = []
    issues_detected = []
    # Initialize deletion counters
    deleted_temp_files = 0
    deleted_empty_dirs = 0

    # --- Define Exclusions ---
    exclude_dirs_patterns = {
        ".git",
        "__pycache__",
        ".venv",
        "venv",
        "node_modules",
        "backups",
        "logs",
    }
    # We handle logs separately

    # --- 1. Clean Temporary Files ---
    temp_patterns = ["*.tmp", "*.bak", "*~"]  # Common temporary file patterns
    found_temp_files = []

    # Scan project root recursively
    for item in PROJECT_ROOT.rglob("*"):
        # Check if item or its parents are in the exclude list
        if any(
            part in exclude_dirs_patterns
            for part in item.relative_to(PROJECT_ROOT).parts
        ):
            continue
        # Also check top-level item itself
        if item.parent == PROJECT_ROOT and item.name in exclude_dirs_patterns:
            continue

        if item.is_file():
            for pattern in temp_patterns:
                if item.match(pattern):
                    found_temp_files.append(item)
                    logger.warning(
                        f"Found temporary file: {
                            item.relative_to(PROJECT_ROOT)}")
                    issues_detected.append(
                        f"Found temporary file: `{
                            item.relative_to(PROJECT_ROOT)}`")
                    # Delete temporary file
                    try:
                        item.unlink()
                        logger.info(
                            f"Deleted temporary file: {
                                item.relative_to(PROJECT_ROOT)}")
                        deleted_temp_files += 1
                    except Exception as e:
                        logger.error(
                            f"Failed to delete temporary file {item}: {e}")
                        issues_detected.append(
                            f"Failed to delete temporary file: `{
                                item.relative_to(PROJECT_ROOT)}` ({e})")
                    break

    if found_temp_files:
        actions_taken.append(
            f"- Temporary file check: Found {len(found_temp_files)}, deleted {deleted_temp_files}."
        )
    warnings_found += len(found_temp_files)

    # --- 2. Clean Empty Directories ---
    found_empty_dirs = []

    # We need to walk the tree bottom-up to safely delete empty dirs
    for dirpath, dirnames, filenames in os.walk(PROJECT_ROOT, topdown=False):
        current_dir = Path(dirpath)

        # Skip excluded directories
        rel_path_parts = current_dir.relative_to(PROJECT_ROOT).parts
        if any(part in exclude_dirs_patterns for part in rel_path_parts):
            continue
        # Check top level dir name itself too
        if not rel_path_parts and current_dir.name in exclude_dirs_patterns:
            continue

        # Check if the directory is empty (only contains other excluded items
        # perhaps)
        try:
            # List items not in exclude list
            non_excluded_items = [
                item
                for item in current_dir.iterdir()
                if item.name not in exclude_dirs_patterns and
                # Also check for pattern matches like .git* ? Maybe not needed
                # for dirs.
                item.name != ".DS_Store"  # Exclude macOS specific
            ]
            # Consider a directory empty if it contains no non-excluded items
            # Or if it only contains empty subdirectories that we are about to delete?
            # Simple check for now: empty if iterdir() list is empty or only
            # contains excluded names
            is_effectively_empty = True
            if not non_excluded_items:
                # Truly empty or only contains excluded names
                pass
            else:
                # Check if it only contains other *empty* directories (which os.walk bottom-up handles)
                # A simpler check: is it just empty now?
                # If Get-ChildItem equivalent is empty, it's empty.
                # Python's os.listdir() or Path.iterdir() might be enough
                # Let's rely on os.walk(topdown=False) - if we reach here and it's empty, it should be safe?
                # Need to be careful not to delete dirs with hidden files we didn't exclude
                # Recheck after potential sub-dir deletions? No, `topdown=False` handles this.
                # Let's check if os.listdir is empty AFTER filtering exclusions
                if not list(
                        current_dir.iterdir()):  # Check if literally empty first
                    pass  # Definitely empty
                elif not non_excluded_items:  # Check if only contains excluded items
                    pass  # Effectively empty in our context
                else:
                    is_effectively_empty = False

            if is_effectively_empty:
                # Check against a list of essential dirs that should never be deleted even if empty
                # Project root should never be deleted
                essential_dirs = {PROJECT_ROOT}
                # Add other essential dirs if needed e.g. PROJECT_ROOT / 'src'?
                if current_dir in essential_dirs:
                    continue

                # Check if it's a python package directory (contains __init__.py)
                # Even empty package dirs might be needed.
                if (current_dir / "__init__.py").exists():
                    logger.debug(
                        f"Skipping potentially empty Python package dir: {
                            current_dir.relative_to(PROJECT_ROOT)}")
                    continue

                # Delete empty directory
                relative_dir_path = current_dir.relative_to(PROJECT_ROOT)
                try:
                    current_dir.rmdir()
                    logger.info(
                        f"Deleted empty directory: {relative_dir_path}")
                    deleted_empty_dirs += 1
                except Exception as e:
                    logger.warning(
                        f"Failed to delete empty directory {relative_dir_path}: {e}")
                    issues_detected.append(
                        f"Failed to delete empty directory: `{relative_dir_path}` ({e})")
                found_empty_dirs.append(current_dir)
        except FileNotFoundError:
            # Directory might have been deleted by a previous step (e.g.,
            # parent deletion)
            continue
        except Exception as e:
            logger.error(f"Error checking directory {current_dir}: {e}")
            issues_detected.append(
                f"Error checking directory: `{
                    current_dir.relative_to(PROJECT_ROOT)}` ({e})")

    if found_empty_dirs:
        actions_taken.append(
            f"- Empty directory check: Found {len(found_empty_dirs)}, deleted {deleted_empty_dirs}."
        )
    warnings_found += len(found_empty_dirs)

    # --- 3. Log File Management ---
    stray_logs = []
    archived_logs = 0
    log_archive_dir = LOG_DIR / "archive"
    log_retention_days = 30  # Keep logs for 30 days
    now_ts = datetime.now().timestamp()

    # Create archive directory if it doesn't exist
    try:
        log_archive_dir.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logger.error(
            f"Failed to create log archive directory {log_archive_dir}: {e}")
        issues_detected.append(f"Failed to create log archive directory: {e}")
        cleanup_success = False

    # Scan for stray logs and archive old logs
    for item in PROJECT_ROOT.rglob("*"):
        # Check if item or its parents are in the exclude list (important to exclude node_modules etc)
        # Reuse exclude_dirs_patterns but allow scanning inside 'logs' dir
        # itself
        relative_path_parts = item.relative_to(PROJECT_ROOT).parts
        is_in_excluded = any(
            part in exclude_dirs_patterns for part in relative_path_parts
        )

        # Special handling for log files
        if item.is_file() and item.suffix.lower() == ".log":
            is_in_log_dir = item.parent == LOG_DIR
            is_in_archive = log_archive_dir in item.parents

            # a) Check for stray logs (outside logs/ and logs/archive/)
            if not is_in_log_dir and not is_in_archive and not is_in_excluded:
                stray_logs.append(item)
                logger.warning(
                    f"Found stray log file outside designated log directories: {
                        item.relative_to(PROJECT_ROOT)}")
                issues_detected.append(
                    f"Found stray log file: `{item.relative_to(PROJECT_ROOT)}`"
                )
                # Move stray log to LOG_DIR
                try:
                    dest = LOG_DIR / item.name
                    shutil.move(str(item), str(dest))
                    logger.info(
                        f"Moved stray log to {
                            dest.relative_to(PROJECT_ROOT)}")
                except Exception as e:
                    logger.error(f"Failed to move stray log {item.name}: {e}")
                    issues_detected.append(
                        f"Failed to move stray log: `{
                            item.relative_to(PROJECT_ROOT)}` ({e})")

            # b) Check logs inside the main LOG_DIR for archiving
            elif is_in_log_dir:
                try:
                    file_mod_time = item.stat().st_mtime
                    if (now_ts - file_mod_time) > (
                        log_retention_days * 86400
                    ):  # 86400 seconds in a day
                        # Archive this old log file
                        log_archive_dir / item.name
                        logger.info(
                            f"Archiving old log file: {
                                item.name} to {
                                log_archive_dir.name}/")
                        # --- Optional: Move old logs ---
                        # Uncomment to enable archiving
                        # try:
                        #     shutil.move(str(item), str(target_archive_path))
                        #     archived_logs += 1
                        # except Exception as move_e:
                        #     logger.error(f"Failed to archive log file {item.name}: {move_e}")
                        #     issues_detected.append(f"Failed to archive log: `{item.name}` ({move_e})")
                except Exception as stat_e:
                    logger.error(
                        f"Could not get status for log file {
                            item.name}: {stat_e}")
                    issues_detected.append(
                        f"Error checking log file status: `{
                            item.name}` ({stat_e})")

    # Update report section for logs
    log_actions = []
    if stray_logs:
        log_actions.append(
            f"- Stray log check: Found {len(stray_logs)}. Move action disabled."
        )
        warnings_found += len(stray_logs)
    else:
        log_actions.append("- Stray log check: No stray log files found.")

    log_actions.append(
        f"- Log archiving: Checked logs older than {log_retention_days} days. Move action disabled. Archived count (if enabled): {archived_logs}."
    )
    # Append log actions to the main actions_taken list or report separately?
    actions_taken.extend(log_actions)

    # --- Update Report ---
    if not issues_detected:
        report_content += "âœ… No major cleanup issues detected." + NL
    else:
        report_content += f"âš ï¸ Found {
            len(issues_detected)} cleanup issues:{NL}"
        for issue in issues_detected:
            report_content += f"  - {issue}{NL}"
        cleanup_success = False  # Mark as failed if issues found
        warnings_found += len(issues_detected)

    report_content += f"{NL}Actions Performed:{NL}" + NL.join(actions_taken)
    update_report("Project Cleanup", report_content)

    # Store cleanup results to memory
    if 'mcp_tools' in globals():
        cleanup_stats = {
            "timestamp": datetime.now().isoformat(),
            "success": cleanup_success,
            "deleted_temp_files": deleted_temp_files,
            "deleted_empty_dirs": deleted_empty_dirs,
            "stray_logs_found": len(stray_logs),
            "archived_logs": archived_logs,
            "total_issues": len(issues_detected),
            "actions_taken": actions_taken,
            "issues_detected": issues_detected
        }
        mcp_tools.store_memory(
            "cleanup_results",
            cleanup_stats,
            "project_cleanup")

    logger.info("Project cleanup check finished.")
    return cleanup_success


# --- New Project Cleanup Function End ---

# --- Final Report Sections Start ---


def new_completion_checklist():
    """Adds the work completion checklist to the report."""
    logger.info("Generating completion checklist...")
    checklist_content = """\
## Work Completion Checklist

Please check the following items after completion, fill in the date and signature

- [ ] Reviewed and resolved all code quality issues
- [ ] Cleaned up the working directory, removed or moved scattered files
- [ ] Checked project directory structure, conforms to specifications
- [ ] Backed up project code
- [ ] Updated project documentation
- [ ] Committed all changes to the version control system
- [ ] Notified relevant team members that the work is complete

**Completion Date**: ________________
**Developer Signature**: ________________

"""
    update_report("Work Completion Checklist", checklist_content)


def generate_summary():
    """Adds the final execution summary to the report."""
    logger.info("Generating execution summary...")
    summary_content = """\
## Execution Summary

- **Total Issues Found**: {issues_found}
- **Total Warnings Found**: {warnings_found}
- **Completion Time**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

### Next Steps

1. Please review all issues and warnings in this report.
2. Resolve all marked issues.
3. Complete all items in the Work Completion Checklist.
4. If necessary, rerun this script to confirm all issues are resolved.

Report generated on: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
    # Append directly to the file, similar to the PS script
    try:
        with open(REPORT_FILE, "a", encoding="utf-8") as f:
            f.write(f"{NL}{NL}{summary_content}")
        logger.info(f"Execution summary appended to report: {REPORT_FILE}")
        return True
    except Exception as e:
        logger.error(f"Failed to append summary to report: {e}")
        return False


# --- Final Report Sections End ---

# --- Code Quality Check Functions Start ---


def invoke_flake8_check():
    """Runs flake8 check on Python code and returns results."""
    global issues_found
    logger.info("Performing Python code quality check (flake8)...")

    flake8_installed, _ = test_flake8()  # Reuse dependency check
    if not flake8_installed:
        # Attempt to install flake8 (consider adding a flag to disable
        # auto-install)
        logger.info("flake8 not detected, attempting to install...")
        stdout, stderr, code = run_command(
            [sys.executable, "-m", "pip", "install", "flake8"]
        )
        if code != 0:
            logger.error(
                f"Failed to install flake8 automatically. Please install it manually. Stderr: {stderr}")
            return (
                False,
                "flake8 not installed and auto-install failed",
                0,
                None,
            )
        # Re-test after install attempt
        flake8_installed, _ = test_flake8()
        if not flake8_installed:
            logger.error("flake8 installation confirmed failed.")
            return False, "flake8 installation failed", 0, None

    config_file = PROJECT_ROOT / "config" / "dev-tools" / ".flake8"
    # å›é€€åˆ°æ ¹ç›®å½• .flake8ï¼Œå½“ dev-tools ä¸‹è¯¥æ–‡ä»¶ä¸å­˜åœ¨æ—¶
    if not config_file.exists():
        config_file = PROJECT_ROOT / ".flake8"
    src_dir = PROJECT_ROOT / "src" / "backend"
    scripts_dir = PROJECT_ROOT / "scripts"
    output_file = LOG_DIR / f"python_check_py_{TIMESTAMP}.log"

    targets = []
    if src_dir.exists() and src_dir.is_dir():
        targets.append(str(src_dir))
    if scripts_dir.exists() and scripts_dir.is_dir():
        # Exclude self if running from scripts dir? Or specific subdirs?
        # For now, include all python files found recursively under scripts
        # Be mindful of virtualenvs if they exist under scripts
        targets.append(str(scripts_dir))

    if not targets:
        logger.info(
            "No Python target directories found (src/backend, scripts). Skipping flake8 check."
        )
        return True, "No target directories found", 0, None

    command = [sys.executable, "-m", "flake8"]
    # Add --isolated flag to try and bypass potential config caching issues
    command.append("--isolated")
    if config_file.exists():
        command.extend(["--config", str(config_file)])
    command.extend(targets)

    logger.info(f"Running flake8 command: {' '.join(command)}")
    # Don't use check=True, as flake8 returns non-zero for issues
    stdout, stderr, code = run_command(command)

    # Write output to log file
    full_output = f"Command: {
        ' '.join(command)}{NL}Return Code: {code}{NL}Stdout:{NL}{stdout}{NL}Stderr:{NL}{stderr}"
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(full_output)
    except Exception as e:
        logger.error(f"Failed to write flake8 output log: {e}")

    # Analyze results (flake8 usually outputs issues to stdout)
    issue_count = 0
    if stdout:
        # Count non-empty lines as issues (simplistic)
        lines = [line for line in stdout.splitlines() if line.strip()]
        issue_count = len(lines)

    if code == 0 and issue_count == 0:
        logger.info("flake8 check completed, no issues found.")
        return True, "No issues found", 0, output_file
    elif issue_count > 0:
        logger.warning(
            f"flake8 check completed, found {issue_count} issues. Details in: {output_file}")
        issues_found += issue_count  # Accumulate script-level counter
        return True, f"Found {issue_count} issues", issue_count, output_file
    else:  # code != 0 but no issues found in stdout? Could be config error.
        logger.error(
            f"flake8 check failed or reported errors (code: {code}). Details in: {output_file}")
        issues_found += 1  # Count execution error as one issue
        return False, f"flake8 execution failed (code: {code})", 0, output_file


def invoke_eslint_check():
    """Runs eslint check on JS/TS code and returns results."""
    global issues_found
    logger.info(
        "Performing JavaScript/TypeScript code quality check (eslint)...")

    eslint_installed, eslint_version = test_eslint()
    logger.info(
        f"test_eslint() returned: installed={eslint_installed}, version={eslint_version}")

    if not eslint_installed:
        # Skip auto-install to prevent unwanted npm package installations
        logger.warning("ESLint not detected. Skipping JavaScript/TypeScript check.")
        logger.info("To enable ESLint checks, please install ESLint manually: npm install eslint --save-dev")
        return (
            True,
            "ESLint not installed - check skipped",
            0,
            None,
        )

    logger.info(f"ESLint detected: {eslint_version}")

    # Use ESLint Flat Config at project root
    config_file = PROJECT_ROOT / "eslint.config.cjs"
    src_dir = PROJECT_ROOT / "src" / "frontend"
    output_file = LOG_DIR / f"js_check_py_{TIMESTAMP}.log"

    if not src_dir.exists() or not src_dir.is_dir():
        logger.info(
            f"JavaScript source directory not found: {src_dir}. Skipping eslint check.")
        return True, "Source directory not found", 0, None

    # Build ESLint command - prioritize npx for consistency with test_eslint
    command = []
    # First try npx eslint (global/registry)
    logger.info("Using npx eslint (prioritized for consistency)...")
    command = ["npx", "eslint"]
    if config_file.exists():
        command.extend(["--config", str(config_file)])
    command.extend(["--ext", ".js,.jsx,.ts,.tsx", str(src_dir)])

    logger.info(f"Running eslint command: {command}")
    logger.info(f"Working directory: {PROJECT_ROOT}")

    # Don't use check=True. Exit code 1 means linting errors found.
    stdout, stderr, code = run_command(command, cwd=str(PROJECT_ROOT))

    logger.info(f"ESLint command completed with exit code: {code}")
    logger.info(f"ESLint stdout length: {len(stdout) if stdout else 0}")
    logger.info(f"ESLint stderr length: {len(stderr) if stderr else 0}")
    if stdout:
        logger.debug(f"ESLint stdout: {stdout[:500]}...")  # åªè®°å½•å‰500å­—ç¬¦
    if stderr:
        logger.debug(f"ESLint stderr: {stderr[:500]}...")  # åªè®°å½•å‰500å­—ç¬¦

    # Write output to log file
    full_output = f"Command: {command}{NL}Return Code: {code}{NL}Stdout:{NL}{stdout}{NL}Stderr:{NL}{stderr}"
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(full_output)
        logger.info(f"ESLint output written to: {output_file}")
    except Exception as e:
        logger.error(f"Failed to write eslint output log: {e}")

    # Analyze results
    # Exit code 0: No errors. Exit code 1: Linting errors found. Exit code >
    # 1: Config/runtime error.
    issue_count = 0
    if code > 1:
        logger.error(
            f"ESLint execution failed with configuration/runtime error (code: {code}). Details in: {output_file}"
        )
        issues_found += 1  # Count execution error as one issue
        return False, f"ESLint execution failed (code: {code})", 0, output_file
    elif code == 1:
        logger.warning(
            f"ESLint check completed and found linting issues. Details in: {output_file}")
        # Try to parse issue count from stdout (ESlint format varies)
        lines = stdout.splitlines()
        for line in reversed(lines):
            if "problems (" in line:
                try:
                    issue_count = int(line.split(
                        "problems (")[0].strip().split()[-1])
                    logger.info(f"Parsed issue count: {issue_count}")
                    break
                except Exception:
                    logger.warning(
                        "Could not parse exact issue count from ESLint summary line."
                    )
                    break  # Stop searching
        if issue_count == 0:
            # Fallback: if code is 1, assume at least one issue
            issue_count = 1
            logger.warning(
                "Could not parse issue count, assuming at least 1 based on exit code."
            )
        issues_found += issue_count
        return True, f"Found {issue_count} issues", issue_count, output_file
    else:  # code == 0
        logger.info("ESLint check completed, no issues found.")
        return True, "No issues found", 0, output_file


def invoke_ci_check(port=3000):
    """Performs CI checks: lint fix, lint, build, serve preview, and availability test."""
    logger.info(f"Starting CI checks on frontend (port {port})...")
    success = True
    # 1. Change to frontend admin directory
    frontend_dir = PROJECT_ROOT / "src" / "frontend" / "admin"
    if not frontend_dir.exists():
        logger.error(f"Frontend directory not found: {frontend_dir}")
        return False
    # Commands to run
    cmds = [
        (["npm", "run", "lint:fix"], "lint:fix"),
        (["npm", "run", "lint"], "lint"),
        (["npm", "run", "build"], "build"),
    ]
    for cmd, name in cmds:
        logger.info(f"Running {name}...")
        out, err, code = run_command(cmd, check=False, cwd=str(frontend_dir))
        if code != 0:
            logger.error(f"{name} failed (code {code}): {err}")
            return False
        logger.info(f"{name} succeeded.")
    # 4. Start serve preview
    serve_cmd = ["npx", "serve", "-s", "build", "-l", str(port)]
    proc = subprocess.Popen(serve_cmd, cwd=str(frontend_dir))
    time.sleep(3)
    # 5. Test availability
    try:
        url = f"http://localhost:{port}"
        resp = urllib.request.urlopen(url, timeout=10)
        if resp.status == 200:
            logger.info("Preview page available: HTTP 200 OK")
        else:
            logger.error(f"Page returned status {resp.status}")
            success = False
    except Exception as e:
        logger.error(f"Page access failed: {e}")
        success = False
    # 6. Close preview service
    try:
        proc.terminate()
        proc.wait(timeout=5)
        logger.info("Preview service stopped.")
    except Exception:
        proc.kill()
        logger.info("Preview service killed.")
    return success


def invoke_code_quality_check(skip_check=False):
    """Runs all code quality checks and updates the report."""
    global issues_found  # We modify the global counter directly here
    logger.info("Starting code quality check...")

    if skip_check:
        logger.info("Skipping code quality check based on parameter.")
        update_report(
            "Code Quality Check",
            "âš ï¸ Skipping code quality check based on parameter.",
        )
        return True  # Skipping is considered success for the flow

    # Initialize results for reporting
    results = {
        "Python (flake8)": {
            "success": False,
            "message": "Not run",
            "issues": 0,
            "output_file": None,
        },
        "JavaScript (eslint)": {
            "success": False,
            "message": "Not run",
            "issues": 0,
            "output_file": None,
        },
    }
    overall_success = True  # Tracks if checks *ran* successfully, not if they passed

    # Run Flake8
    try:
        f_success, f_msg, f_issues, f_outfile = invoke_flake8_check()
        results["Python (flake8)"] = {
            "success": f_success,
            "message": f_msg,
            "issues": f_issues,
            "output_file": f_outfile,
        }
        if not f_success:
            overall_success = False
    except Exception as e:
        logger.error(f"Unexpected error during flake8 check: {e}")
        results["Python (flake8)"]["message"] = f"Execution Error: {e}"
        overall_success = False
        issues_found += 1

    # Run ESLint
    try:
        e_success, e_msg, e_issues, e_outfile = invoke_eslint_check()
        results["JavaScript (eslint)"] = {
            "success": e_success,
            "message": e_msg,
            "issues": e_issues,
            "output_file": e_outfile,
        }
        if not e_success:
            overall_success = False  # Mark if eslint execution failed
    except Exception as e:
        logger.error(f"Unexpected error during eslint check: {e}")
        results["JavaScript (eslint)"]["message"] = f"Execution Error: {e}"
        overall_success = False
        issues_found += 1

    # Generate report section
    report_content = f"### Code Quality Check Results{NL}{NL}"
    report_content += f"| Language Tool       | Status                     | Issues | Details      |{NL}"
    report_content += f"|---------------------|----------------------------|--------|--------------|{NL}"

    for tool_name, res in results.items():
        status_icon = (
            "âœ…"
            if res["issues"] == 0 and res["success"]
            else ("âš ï¸" if res["success"] else "âŒ")
        )
        message = res["message"]
        issues = res["issues"]
        details = "-"
        if res["issues"] > 0 and res["output_file"]:
            rel_path = res["output_file"].relative_to(PROJECT_ROOT).as_posix()
            details = f"[View Log]({rel_path})"
        elif not res["success"]:
            details = message  # Show error message if execution failed
            message = "Execution Failed"

        report_content += (
            f"| {tool_name} | {status_icon} {message} | {issues} | {details} |{NL}")

    update_report("Code Quality Check", report_content)

    # Store code quality check results to memory
    if 'mcp_tools' in globals():
        quality_check_stats = {
            "timestamp": datetime.now().isoformat(),
            "overall_success": overall_success,
            "total_issues_found": issues_found,
            "results": results,
            "tools_checked": list(results.keys())
        }
        mcp_tools.store_memory(
            "code_quality_results",
            quality_check_stats,
            "code_quality")

    if overall_success:
        if issues_found > 0:
            logger.warning(
                f"Code quality checks completed. Total issues found: {issues_found}")
        else:
            logger.info("Code quality checks completed. No issues found.")
    else:
        logger.error(
            "Code quality check execution failed or partially failed.")

    # Return True if the *checks* ran without execution errors,
    # regardless of finding linting issues.
    return overall_success


# --- Code Quality Check Functions End ---


# --- Error Path Detection Integration ---
def invoke_error_path_detection():
    """æ‰§è¡Œé”™è¯¯è·¯å¾„æ£€æµ‹å’Œæ¸…ç†"""
    global issues_found, warnings_found

    logger.info("å¼€å§‹æ‰§è¡Œé”™è¯¯è·¯å¾„æ£€æµ‹å’Œæ¸…ç†...")

    try:
        # å¯¼å…¥é”™è¯¯æ£€æµ‹æ¨¡å—
        from error_path_detector import run_error_detection

        # è®¾ç½®æ¸…ç†ç›®å½•
        cleanup_dir = STANDARD_CLEANUP_DIR

        # æ‰§è¡Œé”™è¯¯æ£€æµ‹
        success, report_file = run_error_detection(
            project_root=PROJECT_ROOT,
            cleanup_dir=cleanup_dir,
            move_files=True,  # è‡ªåŠ¨ç§»åŠ¨é”™è¯¯æ–‡ä»¶
            logger=logger
        )

        if success:
            logger.info(f"âœ… é”™è¯¯è·¯å¾„æ£€æµ‹å’Œæ¸…ç†å®Œæˆï¼ŒæŠ¥å‘Š: {report_file}")
            update_report(
                "é”™è¯¯è·¯å¾„æ£€æµ‹",
                "âœ… **æˆåŠŸå®Œæˆ**\n\n"
                f"- æ£€æµ‹èŒƒå›´: {PROJECT_ROOT}\n"
                f"- æ¸…ç†ç›®å½•: {cleanup_dir}\n"
                f"- è¯¦ç»†æŠ¥å‘Š: `{report_file}`\n\n"
                "é”™è¯¯æ–‡ä»¶å·²è‡ªåŠ¨ç§»åŠ¨åˆ°æ¸…ç†ç›®å½•ï¼Œé¡¹ç›®ä¿æŒæ•´æ´çŠ¶æ€ã€‚"
            )

            # Store successful error detection results to memory
            if 'mcp_tools' in globals():
                mcp_tools.store_memory("error_detection_results", {
                    "timestamp": datetime.now().isoformat(),
                    "success": True,
                    "project_root": str(PROJECT_ROOT),
                    "cleanup_dir": str(cleanup_dir),
                    "report_file": str(report_file)
                }, "error_detection")

            return True
        else:
            logger.warning(f"âš ï¸ é”™è¯¯è·¯å¾„æ£€æµ‹éƒ¨åˆ†å¤±è´¥ï¼ŒæŠ¥å‘Š: {report_file}")
            warnings_found += 1
            update_report(
                "é”™è¯¯è·¯å¾„æ£€æµ‹",
                "âš ï¸ **éƒ¨åˆ†å¤±è´¥**\n\n"
                f"- æ£€æµ‹èŒƒå›´: {PROJECT_ROOT}\n"
                f"- æ¸…ç†ç›®å½•: {cleanup_dir}\n"
                f"- è¯¦ç»†æŠ¥å‘Š: `{report_file}`\n\n"
                "æ£€æµ‹å®Œæˆä½†æ–‡ä»¶ç§»åŠ¨å¯èƒ½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥æ¸…ç†ç›®å½•ã€‚"
            )

            # Store partial failure results to memory
            if 'mcp_tools' in globals():
                mcp_tools.store_memory("error_detection_results", {
                    "timestamp": datetime.now().isoformat(),
                    "success": False,
                    "project_root": str(PROJECT_ROOT),
                    "cleanup_dir": str(cleanup_dir),
                    "report_file": str(report_file),
                    "failure_reason": "partial_failure"
                }, "error_detection")

            return False

    except ImportError as e:
        logger.error(f"æ— æ³•å¯¼å…¥é”™è¯¯æ£€æµ‹æ¨¡å—: {e}")
        issues_found += 1
        update_report(
            "é”™è¯¯è·¯å¾„æ£€æµ‹",
            "âŒ **æ¨¡å—å¯¼å…¥å¤±è´¥**\n\n"
            f"é”™è¯¯: {e}\n\n"
            "è¯·ç¡®ä¿ error_path_detector.py æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®ã€‚"
        )

        # Store import error to memory
        if 'mcp_tools' in globals():
            mcp_tools.store_memory("error_detection_failure", {
                "timestamp": datetime.now().isoformat(),
                "error_type": "ImportError",
                "error_message": str(e),
                "failure_reason": "module_import_failed"
            }, "error_detection")

        return False

    except Exception as e:
        logger.error(f"é”™è¯¯è·¯å¾„æ£€æµ‹æ‰§è¡Œå¤±è´¥: {e}")
        issues_found += 1
        update_report(
            "é”™è¯¯è·¯å¾„æ£€æµ‹",
            "âŒ **æ‰§è¡Œå¤±è´¥**\n\n"
            f"é”™è¯¯: {e}\n\n"
            "è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚"
        )

        # Store execution error to memory
        if 'mcp_tools' in globals():
            mcp_tools.store_memory("error_detection_failure", {
                "timestamp": datetime.now().isoformat(),
                "error_type": "Exception",
                "error_message": str(e),
                "failure_reason": "execution_failed"
            }, "error_detection")

        return False


# --- Main Execution ---
def main():
    """Main function to run the work completion process with TaskManager integration."""
    # ä½¿ç”¨å…¨å±€å·²è§£æçš„argså‚æ•°

    logger.info("Starting finish.py script with TaskManager integration...")

    # å­˜å‚¨è„šæœ¬å¯åŠ¨ä¿¡æ¯åˆ°memory
    mcp_tools.store_memory("script_start", {
        "timestamp": TIMESTAMP,
        "mode": "daily" if args.daily else "release" if args.release else "backup_only" if args.backup_only else "default",
        "args": vars(args)
    })

    # å¿«é€Ÿå¤‡ä»½æ¨¡å¼ï¼šä»…æ‰§è¡Œå¤‡ä»½ï¼Œè·³è¿‡å…¶ä»–æ‰€æœ‰æ­¥éª¤
    if args.backup_only:
        logger.info("å¿«é€Ÿå¤‡ä»½æ¨¡å¼ï¼šä»…æ‰§è¡Œé¡¹ç›®å¤‡ä»½")
        backup_ok = invoke_backup(False)  # å¼ºåˆ¶æ‰§è¡Œå¤‡ä»½
        if backup_ok:
            logger.info("å¿«é€Ÿå¤‡ä»½å®Œæˆï¼")
            mcp_tools.store_memory(
                "backup_result", {
                    "status": "success", "mode": "backup_only"})
            sys.exit(0)
        else:
            logger.error("å¿«é€Ÿå¤‡ä»½å¤±è´¥ï¼")
            mcp_tools.store_memory(
                "backup_result", {
                    "status": "failed", "mode": "backup_only"})
            sys.exit(1)

    # è‡ªåŠ¨æ ¼å¼åŒ–é˜¶æ®µ
    invoke_autoformat()
    # 1. Initialize Report
    if not initialize_report():
        logger.error("Failed to initialize report. Aborting.")
        sys.exit(1)

    # 2. Check Dependencies
    if not test_dependencies():
        logger.error("Dependency check failed for required tools. Aborting.")
        # TODO: Call a summary function before exiting?
        update_report(
            "Execution Aborted",
            "Dependency check failed, cannot continue.")
        sys.exit(1)
    logger.info("Dependency check passed.")

    # 3. Directory Structure Check (Simplified)
    if not invoke_directory_check():
        logger.error(
            "Simplified directory check failed. Aborting (in this test phase)."
        )
        update_report(
            "Execution Aborted",
            "Simplified directory check failed.")
        sys.exit(1)
    logger.info("Simplified directory check passed.")

    # --- Add calls to other checks here later ---
    # invoke_code_quality_check(args.no_quality_check)
    code_quality_ok = invoke_code_quality_check(args.no_quality_check)
    if not code_quality_ok:
        logger.warning(
            "Code quality check execution failed. Continuing cautiously...")
        # Decide if this should abort or just warn

    # invoke_workdir_clean_check()
    workdir_clean_ok = invoke_workdir_clean_check()
    if not workdir_clean_ok:
        logger.warning(
            "Working directory cleanliness check found scattered items.")
        # This is usually a warning, not a failure for the overall process

    # æ‰§è¡Œé¡¹ç›®æ¸…ç†
    cleanup_ok = invoke_project_cleanup_py()
    if not cleanup_ok:
        logger.warning("Project cleanup identified issues or failed.")
    # Agent è‡ªåŠ¨æ£€æŸ¥ç›®å½•å’Œæ–‡ä»¶ç»“æ„ - éœ€è¦æƒé™æ§åˆ¶å’Œé€»è¾‘æ¨ç†
    logger.info("å‡†å¤‡æ‰§è¡Œç›®å½•ç»“æ„éªŒè¯ï¼Œå¼€å§‹é€»è¾‘æ¨ç†åˆ†æ...")

    # Sequential thinking for directory structure verification strategy
    structure_decision_points = [
        {
            "question": "å¦‚ä½•ç¡®ä¿ç›®å½•ç»“æ„éªŒè¯çš„å‡†ç¡®æ€§ï¼Ÿ",
            "factors": ["é¡¹ç›®è§„èŒƒè¦æ±‚", "ç°æœ‰æ–‡ä»¶ç»“æ„", "è‡ªåŠ¨åŒ–ç¨‹åº¦"],
            "risks": ["è¯¯åˆ é‡è¦æ–‡ä»¶", "ç»“æ„ç ´å", "æ•°æ®ä¸¢å¤±"],
            "benefits": ["è§„èŒƒåŒ–ç®¡ç†", "æé«˜ç»´æŠ¤æ€§", "å›¢é˜Ÿåä½œæ•ˆç‡"],
            "recommendation": "é‡‡ç”¨æ¸è¿›å¼éªŒè¯ï¼Œå…ˆæ£€æŸ¥åæ¸…ç†ï¼Œç¡®ä¿å®‰å…¨æ€§"
        },
        {
            "question": "æ˜¯å¦éœ€è¦äººå·¥å®¡æ‰¹ä»‹å…¥ï¼Ÿ",
            "factors": ["å†å²é—®é¢˜è®°å½•", "é£é™©è¯„ä¼°", "è‡ªåŠ¨åŒ–ä¿¡ä»»åº¦"],
            "risks": ["è‡ªåŠ¨åŒ–è¯¯æ“ä½œ", "é‡è¦æ–‡ä»¶ä¸¢å¤±", "é¡¹ç›®ç»“æ„ç ´å"],
            "benefits": ["äººå·¥æŠŠå…³", "é™ä½é£é™©", "ç¡®ä¿å‡†ç¡®æ€§"],
            "recommendation": "å¯¹äºå…³é”®ç»“æ„å˜æ›´ï¼Œå¿…é¡»è¿›è¡Œäººå·¥å®¡æ‰¹"
        }
    ]

    structure_thinking = mcp_tools.sequential_thinking(
        context="ç›®å½•ç»“æ„éªŒè¯ç­–ç•¥åˆ†æ",
        decision_points=structure_decision_points,
        reasoning_type="structure_verification"
    )

    # æ£€æŸ¥æ˜¯å¦æœ‰æœªå¤„ç†çš„ç›®å½•ç»“æ„é—®é¢˜éœ€è¦å®¡æ‰¹
    previous_structure_issues = mcp_tools.get_memory("structure_issues")
    if previous_structure_issues and previous_structure_issues.get(
            "status") == "pending_approval":
        logger.warning("å‘ç°æœªå¤„ç†çš„ç›®å½•ç»“æ„é—®é¢˜ï¼Œéœ€è¦äººå·¥å®¡æ‰¹")
        task_id = mcp_tools.create_approval_task(
            title="ç›®å½•ç»“æ„åˆè§„æ€§æ£€æŸ¥å®¡æ‰¹", description=f"å‘ç°ç›®å½•ç»“æ„é—®é¢˜éœ€è¦å®¡æ‰¹å¤„ç†ï¼š{
                previous_structure_issues.get(
                    'issues', [])}", priority="high")
        logger.info(f"å·²åˆ›å»ºå®¡æ‰¹ä»»åŠ¡ #{task_id}ï¼Œç­‰å¾…äººå·¥å¤„ç†...")

        # ç­‰å¾…å®¡æ‰¹ï¼ˆåœ¨å®é™…ç¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯å¼‚æ­¥ç­‰å¾…ï¼‰
        approval_result = mcp_tools.wait_for_approval(task_id)
        if not approval_result:
            logger.error("ç›®å½•ç»“æ„å®¡æ‰¹è¢«æ‹’ç»ï¼Œç»ˆæ­¢æ‰§è¡Œ")
            sys.exit(1)
        logger.info("ç›®å½•ç»“æ„å®¡æ‰¹é€šè¿‡ï¼Œç»§ç»­æ‰§è¡Œ")

    agent_ok = agent_verify_structure(
        auto_cleanup=getattr(
            args, 'auto_cleanup', False))
    if not agent_ok:
        logger.error("AgentéªŒè¯: é¡¹ç›®æ ¹ç›®å½•å­˜åœ¨ä¸ç¬¦åˆè§„èŒƒçš„é¡¹ï¼Œç»ˆæ­¢æ‰§è¡Œã€‚")
        # å­˜å‚¨ç»“æ„é—®é¢˜åˆ°memory
        mcp_tools.store_memory("structure_issues", {
            "status": "failed",
            "timestamp": TIMESTAMP,
            "issues": ["é¡¹ç›®æ ¹ç›®å½•å­˜åœ¨ä¸ç¬¦åˆè§„èŒƒçš„é¡¹"]
        })
        sys.exit(1)
    else:
        # å­˜å‚¨æˆåŠŸç»“æœ
        mcp_tools.store_memory("structure_verification", {
            "status": "passed",
            "timestamp": TIMESTAMP
        })

    # æ‰§è¡Œé”™è¯¯è·¯å¾„æ£€æµ‹å’Œæ¸…ç†ï¼ˆæ–°å¢çš„æ—¥å¸¸å·¥ä½œï¼‰
    error_detection_ok = invoke_error_path_detection()
    if not error_detection_ok:
        logger.warning("é”™è¯¯è·¯å¾„æ£€æµ‹éƒ¨åˆ†å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œå…¶ä»–æ­¥éª¤ã€‚")

    # æ‰§è¡Œå¤‡ä»½ - æ·»åŠ é€»è¾‘æ¨ç†åˆ†æ
    logger.info("å‡†å¤‡æ‰§è¡Œé¡¹ç›®å¤‡ä»½ï¼Œå¼€å§‹ç­–ç•¥åˆ†æ...")

    # Sequential thinking for backup strategy
    backup_decision_points = [
        {
            "question": "å¦‚ä½•ç¡®ä¿å¤‡ä»½çš„å®Œæ•´æ€§å’Œå¯é æ€§ï¼Ÿ",
            "factors": ["æ•°æ®é‡è¦æ€§", "å¤‡ä»½é¢‘ç‡", "å­˜å‚¨ç©ºé—´", "æ¢å¤é€Ÿåº¦"],
            "risks": ["æ•°æ®ä¸¢å¤±", "å¤‡ä»½æŸå", "å­˜å‚¨ç©ºé—´ä¸è¶³", "å¤‡ä»½æ—¶é—´è¿‡é•¿"],
            "benefits": ["æ•°æ®å®‰å…¨", "å¿«é€Ÿæ¢å¤", "ç‰ˆæœ¬æ§åˆ¶", "ç¾éš¾æ¢å¤"],
            "recommendation": "é‡‡ç”¨å¢é‡å¤‡ä»½ç­–ç•¥ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§éªŒè¯"
        },
        {
            "question": "å¤‡ä»½å¤±è´¥æ—¶çš„åº”å¯¹ç­–ç•¥ï¼Ÿ",
            "factors": ["å¤±è´¥åŸå› ", "é‡è¯•æœºåˆ¶", "å¤‡ç”¨æ–¹æ¡ˆ", "é€šçŸ¥æœºåˆ¶"],
            "risks": ["æ•°æ®æ°¸ä¹…ä¸¢å¤±", "å·¥ä½œä¸­æ–­", "é¡¹ç›®å»¶æœŸ"],
            "benefits": ["é£é™©æ§åˆ¶", "ä¸šåŠ¡è¿ç»­æ€§", "æ•°æ®ä¿æŠ¤"],
            "recommendation": "å»ºç«‹å¤šå±‚å¤‡ä»½æœºåˆ¶ï¼Œå¤±è´¥æ—¶ç«‹å³å‘Šè­¦å¹¶å¯åŠ¨å¤‡ç”¨æ–¹æ¡ˆ"
        }
    ]

    backup_thinking = mcp_tools.sequential_thinking(
        context="é¡¹ç›®å¤‡ä»½ç­–ç•¥åˆ†æ",
        decision_points=backup_decision_points,
        reasoning_type="backup_strategy"
    )

    backup_ok = invoke_backup(args.no_backup)
    if not backup_ok and not args.no_backup:
        logger.error("Project backup failed.")
        mcp_tools.store_memory("backup_result", {
            "status": "failed",
            "timestamp": TIMESTAMP,
            "mode": "full_process"
        })
    else:
        mcp_tools.store_memory("backup_result", {
            "status": "success" if backup_ok else "skipped",
            "timestamp": TIMESTAMP,
            "mode": "full_process"
        })

    # generate_summary()
    generate_summary()
    # new_completion_checklist()
    new_completion_checklist()

    logger.info("All checks performed.")

    # å­˜å‚¨å®Œæ•´çš„æ‰§è¡Œç»“æœåˆ°memory
    execution_result = {
        "timestamp": TIMESTAMP,
        "issues_found": issues_found,
        "warnings_found": warnings_found,
        "backup_ok": backup_ok,
        "code_quality_ok": code_quality_ok,
        "workdir_clean_ok": workdir_clean_ok,
        "cleanup_ok": cleanup_ok,
        "agent_ok": agent_ok,
        "error_detection_ok": error_detection_ok
    }
    mcp_tools.store_memory("execution_result", execution_result)

    # å¦‚æœæœ‰å…³é”®é—®é¢˜ï¼Œåˆ›å»ºå®¡æ‰¹ä»»åŠ¡
    critical_failures = []
    if not backup_ok and not args.no_backup:
        critical_failures.append("å¤‡ä»½å¤±è´¥")
    if not agent_ok:
        critical_failures.append("ç›®å½•ç»“æ„éªŒè¯å¤±è´¥")

    if critical_failures:
        task_id = mcp_tools.create_approval_task(
            title="å·¥ä½œå®Œæˆæµç¨‹å…³é”®é—®é¢˜å¤„ç†",
            description=f"å‘ç°å…³é”®å¤±è´¥éœ€è¦å¤„ç†ï¼š{', '.join(critical_failures)}",
            priority="critical"
        )
        logger.warning(f"å·²åˆ›å»ºå…³é”®é—®é¢˜å®¡æ‰¹ä»»åŠ¡ #{task_id}")

    # Determine final exit code based on critical failures / issues
    # Example: Fail if dependencies missing or backup failed (when not skipped)
    final_status_success = True
    # Recheck dependencies status (stored from earlier? Or re-run required checks?)
    # For simplicity, let's assume if backup failed, overall process failed
    if not backup_ok and not args.no_backup:
        final_status_success = False
        # Add critical dependency check failure here too if needed

    if final_status_success:
        if issues_found > 0:
            logger.warning(
                f"Work completion process finished with issues ({issues_found}) and warnings ({warnings_found}). Review report.")
            mcp_tools.store_memory("final_status", {
                "status": "completed_with_issues",
                "issues": issues_found,
                "warnings": warnings_found
            })
            sys.exit(0)  # Success exit code, but report indicates issues
        else:
            logger.info(
                f"Work completion process finished successfully with no issues ({warnings_found} warnings).")
            mcp_tools.store_memory("final_status", {
                "status": "success",
                "issues": issues_found,
                "warnings": warnings_found
            })
            sys.exit(0)
    else:
        logger.error(
            f"Work completion process finished with CRITICAL failures. Issues: {issues_found}, Warnings: {warnings_found}. Review report."
        )
        mcp_tools.store_memory("final_status", {
            "status": "critical_failure",
            "issues": issues_found,
            "warnings": warnings_found,
            "critical_failures": critical_failures
        })
        sys.exit(1)  # Failure exit code


if __name__ == "__main__":
    main()
